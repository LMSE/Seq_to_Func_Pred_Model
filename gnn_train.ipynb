{"cells":[{"cell_type":"markdown","source":["## Installation and setup"],"metadata":{"id":"yYAnBQHeY2Z7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"63_ynUxToABG"},"outputs":[],"source":["import torch\n","print(torch.__version__)\n","print(torch.version.cuda)\n","torch_version = torch.__version__\n","cuda_version = torch.version.cuda.replace('.', '')\n","base_url = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJ_MyDZj51Ra"},"outputs":[],"source":["# Install PyTorch Geometric and its dependencies\n","!pip install -q torch-scatter -f $base_url\n","!pip install -q torch-sparse -f $base_url\n","!pip install -q torch-cluster -f $base_url\n","!pip install -q torch-spline-conv -f $base_url\n","!pip install -q torch-geometric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6TlKhim8-nP1"},"outputs":[],"source":["# Install Biopython for PDBParser\n","!pip install biopython"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zi5TBAz3MPd0"},"outputs":[],"source":["import torch.utils.data as data\n","import torch_geometric\n","import torch_cluster\n","import torch.nn.functional as F\n","from Bio.PDB import PDBParser, Polypeptide\n","import numpy as np\n","import tqdm\n","import random\n","import math\n","import re\n","import pickle\n","import torch, functools\n","from torch import nn\n","from torch_geometric.nn import MessagePassing\n","from torch_scatter import scatter_add\n","import torch.nn as nn\n","from torch.distributions import Categorical\n","from torch_scatter import scatter_mean\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","from pathlib import Path\n","import time\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n","from scipy.stats import spearmanr\n","from scipy.stats import pearsonr\n","from scipy import stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhSRNyFXO4ra"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9pIOywkrO8_J"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLON51Fm9fLF"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYmH7t5FYUIq"},"outputs":[],"source":["seeds = [0,1,2,42,1234]\n","seed=seeds[0]\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"markdown","source":["## Helper Functions"],"metadata":{"id":"N4G4CdkxY60W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uKw3jpQJMVJM"},"outputs":[],"source":["def _normalize(tensor, dim=-1):\n","    return torch.nan_to_num(torch.div(tensor, torch.norm(tensor, dim=dim, keepdim=True)))\n","\n","def _rbf(D, D_min=0., D_max=20., D_count=16, device='cpu'):\n","    D_mu = torch.linspace(D_min, D_max, D_count, device=device)\n","    D_mu = D_mu.view([1, -1])\n","    D_sigma = (D_max - D_min) / D_count\n","    D_expand = torch.unsqueeze(D, -1)\n","    RBF = torch.exp(-((D_expand - D_mu) / D_sigma) ** 2)\n","    return RBF"]},{"cell_type":"markdown","source":["## Code to Parse Fasta File of Sequences\n","Modify if the format changes from the FLIP Format of data"],"metadata":{"id":"4wZOiCs9ZEoq"}},{"cell_type":"code","source":["def parse_fasta(fasta_file):\n","    data_info = {}\n","    current_idx = None\n","\n","    with open(fasta_file, 'r') as file:\n","        for line in file:\n","            line = line.strip()  # Remove newline and trailing spaces\n","            if line.startswith('>'):\n","                # Parse the header line\n","                match = re.match(r'>Sequence(\\d+) TARGET=([\\d.]+) SET=(\\w+) VALIDATION=False', line)\n","                if match:\n","                    current_idx, target, set_type = int(match.group(1)), float(match.group(2)), match.group(3)\n","                    data_info[current_idx] = {'target': target, 'set': set_type, 'sequence': ''}\n","            else:\n","                # Read sequence line and append it to the current sequence\n","                if current_idx is not None:\n","                    data_info[current_idx]['sequence'] += line\n","\n","    return data_info"],"metadata":{"id":"jnuta9K5ZDVt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing, Dataset and Dataloader"],"metadata":{"id":"9UB2NDEcYzej"}},{"cell_type":"markdown","source":["The Collate Function is important to create batches of graphs with indices denoting the graph a node originates from"],"metadata":{"id":"b6zeJbadZara"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LclSmNDuiNw"},"outputs":[],"source":["def custom_collate(batch):\n","    # Use torch_geometric.data.Batch.from_data_list to handle standard attributes\n","    # create batches of graphs with indices denoting the graph a node originates from\n","    batched_data = torch_geometric.data.Batch.from_data_list(batch)\n","\n","    # Additional attributes (target, seq_tensor, mask, name) need to be handled separately\n","    targets = torch.tensor([item.target for item in batch], dtype=torch.float32)\n","    names = [item.name for item in batch]\n","    seq_tensors = torch.cat([item.seq for item in batch], dim=0)\n","    masks = torch.cat([item.mask for item in batch], dim=0)\n","\n","    # Add these attributes to the batched_data\n","    batched_data.target = targets\n","    batched_data.name = names\n","    batched_data.seq = seq_tensors\n","    batched_data.mask = masks\n","\n","    return batched_data"]},{"cell_type":"markdown","source":["Batchsampler will assign the number of nodes in each batch and shuffle the graphs.\n","\n","Number of graphs in a batch * Number of nodes in a graph <= max_nodes"],"metadata":{"id":"jjAHC4tKZnvC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2R6UXzf9MXIj"},"outputs":[],"source":["class BatchSampler(data.Sampler):\n","    def __init__(self, node_counts, max_nodes=3000, shuffle=True):\n","        # can change max_nodes to change batch size\n","        self.node_counts = node_counts\n","        self.idx = [i for i in range(len(node_counts)) if node_counts[i] <= max_nodes]\n","        self.shuffle = shuffle\n","        self.max_nodes = max_nodes\n","        self._form_batches()\n","\n","    def _form_batches(self):\n","        self.batches = []\n","        if self.shuffle: random.shuffle(self.idx)\n","        idx = self.idx\n","        while idx:\n","            batch = []\n","            n_nodes = 0\n","            while idx and n_nodes + self.node_counts[idx[0]] <= self.max_nodes:\n","                next_idx, idx = idx[0], idx[1:]\n","                n_nodes += self.node_counts[next_idx]\n","                batch.append(next_idx)\n","            self.batches.append(batch)\n","\n","    def __len__(self):\n","        if not self.batches: self._form_batches()\n","        return len(self.batches)\n","\n","    def __iter__(self):\n","        if not self.batches: self._form_batches()\n","        for batch in self.batches: yield batch"]},{"cell_type":"markdown","source":["### All featurization is done in this class\n","\n","If you're working with less RAM, precompute and save graph features and access them while training/testing. (this code needs to be added)\n","\n","Current code supports two situations:\n","\n","\n","1.   You can create the graph for each protein as and when required when the __getitem__() method is called. While this saves RAM and disk space, it takes a lot of time while training and testing. Ideal for situations when you are training with ESM emebeddings as node features. (comment out the graph_cache related lines)\n","2.   The graphs created for each protein will be saved in the first epoch and then can be accessed using the graph_cache dictionary. This facilitates fast training but takes up RAM and disk space. Ideal for situations when you are training without ESM emebeddings as node features. It should also work when you are training with embeddings as long as you are using A100. (current version of the code works)\n","3. Pre-computing is probably the best optiuon. Need to add code for that.\n","\n"],"metadata":{"id":"gj53GUTMaObH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wvf-yod-Mdxc"},"outputs":[],"source":["class ProteinGraphDataset(data.Dataset):\n","    def __init__(self, pdb_files, data_info, split, use_embeddings=False, embeddings=None, embedding_dim=1280, top_k=32, num_rbf=16, num_positional_embeddings=16, device=\"cpu\"):\n","        self.pdb_files = pdb_files\n","        self.use_embeddings = use_embeddings\n","        if use_embeddings and embeddings is not None:\n","            self.embeddings = embeddings\n","        self.data_info = data_info\n","        self.split = split\n","        self.embedding_dim = embedding_dim\n","        self.top_k = top_k\n","        self.num_rbf = num_rbf\n","        self.num_positional_embeddings = num_positional_embeddings\n","        self.device = device\n","        self.selected_indices = [idx for idx, info in data_info.items() if info['set'] == split]\n","        self.graph_cache = {} #comment this out if you don't want to cache\n","        self.letter_to_num = {'C': 4, 'D': 3, 'S': 15, 'Q': 5, 'K': 11, 'I': 9,\n","                              'P': 14, 'T': 16, 'F': 13, 'A': 0, 'G': 7, 'H': 8,\n","                              'E': 6, 'L': 10, 'R': 1, 'W': 17, 'V': 19,\n","                              'N': 2, 'Y': 18, 'M': 12}\n","\n","    def __len__(self):\n","        return len(self.selected_indices)\n","\n","    def __getitem__(self, idx):\n","        #comment the following 2 lines out if you don't want to cache\n","        if idx in self.graph_cache:\n","            return self.graph_cache[idx]\n","        actual_idx = self.selected_indices[idx]\n","        target = self.data_info[actual_idx]['target']\n","        pdb_file = self.pdb_files[actual_idx]\n","        coords = self._parse_pdb(pdb_file)  # Directly returns coords\n","        # coords, pdb_sequence = self._parse_pdb(pdb_file)\n","        # assert pdb_sequence == self.data_info[actual_idx]['sequence'], f\"Sequence mismatch for index {idx}\"\n","        # print(pdb_sequence == self.data_info[actual_idx]['sequence'])\n","        graph = self._featurize_as_graph(coords, actual_idx)\n","\n","        # Create a Data object to hold both the graph and the target, along with seq and mask\n","        data = torch_geometric.data.Data(\n","            x=graph.x,\n","            seq=graph.seq_tensor,\n","            name = graph.name,\n","            node_s=graph.node_s,\n","            node_v=graph.node_v,\n","            edge_s=graph.edge_s,\n","            edge_v=graph.edge_v,\n","            edge_index=graph.edge_index,\n","            mask=graph.mask,\n","            target=target,  # Include the target in the Data object\n","        )\n","\n","        self.graph_cache[idx] = data #comment this out if you don't want to cache\n","        return data\n","\n","    def _parse_pdb(self, pdb_file):\n","        parser = PDBParser()\n","        structure = parser.get_structure(pdb_file, pdb_file)\n","\n","        # Initializing a dictionary to store the coordinates of each required atom type\n","        coords = {'N': [], 'CA': [], 'C': [], 'O': []}\n","\n","        for model in structure:\n","            for chain in model:\n","                for residue in chain:\n","                    # A temporary storage for the current residue's atoms\n","                    residue_coords = {'N': None, 'CA': None, 'C': None, 'O': None}\n","\n","                    for atom in residue:\n","                        if atom.get_name() in residue_coords:\n","                            residue_coords[atom.get_name()] = atom.get_coord()\n","\n","                    # If all required atoms are present, add their coordinates\n","                    if all(atom in residue_coords and residue_coords[atom] is not None for atom in ['N', 'CA', 'C', 'O']):\n","                        for atom in ['N', 'CA', 'C', 'O']:\n","                            coords[atom].append(residue_coords[atom])\n","\n","        # Convert lists to numpy arrays and then to tensors, maintaining the atom type order\n","        coords = {atom: torch.tensor(np.array(coords[atom]), device=self.device, dtype=torch.float32) for atom in ['N', 'CA', 'C', 'O']}\n","\n","        return coords\n","\n","\n","    def _featurize_as_graph(self, coords, idx):\n","        with torch.no_grad():\n","            # Convert coordinates to tensors and move to the specified device\n","            coords = {k: torch.tensor(v, device=self.device, dtype=torch.float32).clone().detach() for k, v in coords.items()}\n","            seq = [self.letter_to_num.get(amino_acid, -1) for amino_acid in self.data_info[idx]['sequence']]\n","            seq_tensor = torch.tensor(seq, dtype=torch.long, device=self.device)\n","\n","            # Combining all atom coordinates into a single tensor\n","            all_coords = torch.stack([coords['N'], coords['CA'], coords['C'], coords['O']], dim=1)\n","\n","            # Generate mask and update coordinates\n","            mask = torch.isfinite(all_coords.sum(dim=(1, 2)))\n","            all_coords[~mask] = np.inf  # Handle non-finite values\n","\n","            # Extract 'CA' coordinates for graph construction\n","            X_ca = all_coords[:, 1]\n","\n","            # Graph connectivity based on nearest neighbors in the CA coordinates\n","            edge_index = torch_cluster.knn_graph(X_ca, k=self.top_k)\n","\n","            # Edge features\n","            E_vectors = X_ca[edge_index[0]] - X_ca[edge_index[1]]\n","            rbf = _rbf(E_vectors.norm(dim=-1), D_count=self.num_rbf, device=self.device)\n","\n","            # Node features: dihedrals and sequence embeddings\n","            dihedrals = self._dihedrals(coords)\n","            if self.use_embeddings:\n","                sequence_embeddings = self.embeddings[f'Sequence{idx}']\n","                #print(sequence_embeddings.shape)\n","                assert sequence_embeddings.shape[0] == len(coords['CA']), \"Mismatch in number of amino acids in structure and embeddings\"\n","                # Scale sequence embeddings to [-1, 1]\n","                min_val = sequence_embeddings.min()\n","                max_val = sequence_embeddings.max()\n","                sequence_embeddings = 2 * ((sequence_embeddings - min_val) / (max_val - min_val)) - 1\n","                node_s = torch.cat([dihedrals, sequence_embeddings], dim=-1)\n","            else:\n","                node_s = dihedrals\n","\n","            # Additional node features: orientations and sidechains\n","            orientations = self._orientations(X_ca)\n","            sidechains = self._sidechains(coords['N'], coords['CA'], coords['C'])\n","            node_v = torch.cat([orientations, sidechains.unsqueeze(-2)], dim=-2)\n","\n","            # Positional embeddings for edges\n","            pos_embeddings = self._positional_embeddings(edge_index)\n","            edge_s = torch.cat([rbf, pos_embeddings], dim=-1)\n","            edge_v = _normalize(E_vectors).unsqueeze(-2)\n","\n","            # Apply nan_to_num to all features\n","            node_s, node_v, edge_s, edge_v = map(torch.nan_to_num, (node_s, node_v, edge_s, edge_v))\n","\n","        # Create graph data\n","        data = torch_geometric.data.Data(x=X_ca,\n","                                         seq_tensor = seq_tensor,\n","                                         name=f'Sequence{idx}',\n","                                         node_s=node_s,\n","                                         node_v=node_v,\n","                                         edge_s=edge_s,\n","                                         edge_v=edge_v,\n","                                         edge_index=edge_index,\n","                                         mask = mask\n","                                         )\n","        return data\n","\n","\n","    def _dihedrals(self, coords, eps=1e-7):\n","        X = torch.cat([torch.tensor(coords['N'], dtype=torch.float32, device=self.device).clone().detach(),\n","                       torch.tensor(coords['CA'], dtype=torch.float32, device=self.device).clone().detach(),\n","                       torch.tensor(coords['C'], dtype=torch.float32, device=self.device).clone().detach()], dim=0)\n","\n","\n","        dX = X[1:] - X[:-1]\n","        U = _normalize(dX, dim=-1)\n","        u_2 = U[:-2]; u_1 = U[1:-1]; u_0 = U[2:]\n","\n","        n_2 = _normalize(torch.cross(u_2, u_1), dim=-1)\n","        n_1 = _normalize(torch.cross(u_1, u_0), dim=-1)\n","\n","        cosD = torch.sum(n_2 * n_1, -1)\n","        cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n","        D = torch.sign(torch.sum(u_2 * n_1, -1)) * torch.acos(cosD)\n","\n","        D = F.pad(D, [1, 2])\n","        D = torch.reshape(D, [-1, 3])\n","        D_features = torch.cat([torch.cos(D), torch.sin(D)], 1)\n","        return D_features\n","\n","    def _positional_embeddings(self, edge_index, num_embeddings=None, period_range=[2, 1000]):\n","        num_embeddings = num_embeddings or self.num_positional_embeddings\n","        d = edge_index[0] - edge_index[1]\n","\n","        frequency = torch.exp(torch.arange(0, num_embeddings, 2, dtype=torch.float32, device=self.device)\n","                              * -(np.log(10000.0) / num_embeddings))\n","        angles = d.unsqueeze(-1) * frequency\n","        E = torch.cat((torch.cos(angles), torch.sin(angles)), -1)\n","        return E\n","\n","    def _orientations(self, X_ca):\n","        forward = _normalize(X_ca[1:] - X_ca[:-1])\n","        backward = _normalize(X_ca[:-1] - X_ca[1:])\n","        forward = F.pad(forward, [0, 0, 0, 1])\n","        backward = F.pad(backward, [0, 0, 1, 0])\n","        return torch.cat([forward.unsqueeze(-2), backward.unsqueeze(-2)], -2)\n","\n","    def _sidechains(self, N, CA, C):\n","        n = _normalize(torch.tensor(N, dtype=torch.float32, device=self.device) - torch.tensor(CA, dtype=torch.float32, device=self.device))\n","        c = _normalize(torch.tensor(C, dtype=torch.float32, device=self.device) - torch.tensor(CA, dtype=torch.float32, device=self.device))\n","        bisector = _normalize(c + n)\n","        perp = _normalize(torch.cross(c, n))\n","        vec = -bisector * math.sqrt(1 / 3) - perp * math.sqrt(2 / 3)\n","        return vec"]},{"cell_type":"markdown","metadata":{"id":"tkTZF9liT2CJ"},"source":["## Tuple Helper Functions\n","refer to docstrings for help\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0O2FjhC8LNW"},"outputs":[],"source":["def tuple_sum(*args):\n","    '''\n","    Sums any number of tuples (s, V) elementwise.\n","\n","    Parameters:\n","        *args: A variable number of tuples to be summed.\n","\n","    Returns:\n","        A single tuple containing the element-wise sum of all input tuples.\n","\n","    Example:\n","        >>> tuple_sum((1, 2), (3, 4), (5, 6))\n","        (9, 12)\n","\n","        This example demonstrates the addition of three tuples: (1, 2), (3, 4), and (5, 6).\n","        The function sums the first elements of all tuples (1 + 3 + 5) and the second elements (2 + 4 + 6),\n","        resulting in the tuple (9, 12).\n","    '''\n","    return tuple(map(sum, zip(*args)))\n","\n","\n","def tuple_cat(*args, dim=-1):\n","    '''\n","    Concatenates any number of tuples (s, V) elementwise, where each tuple contains two tensors.\n","\n","    Parameters:\n","        *args: A variable number of tuples, where each tuple contains two tensors to be concatenated.\n","        dim (int): The dimension along which to concatenate. For scalar-channel tensors (`s`),\n","                   `dim` refers to the dimension in `s`. For vector-channel tensors (`V`),\n","                   `dim` (when set to -1) is equivalent to `dim=-2`, accounting for the additional\n","                   channel dimension in `V`.\n","\n","    Returns:\n","        A single tuple containing two tensors, which are the concatenations of the first and second elements\n","        from each input tuple, respectively.\n","\n","    Dimension Explanation:\n","        - For a tensor `t` in the scalar-channel tuple (first element), `t[dim]` refers to the dimension along\n","          which the concatenation occurs. If `t` is 1D, `dim=0` would be the length of `t`.\n","        - For a tensor `t` in the vector-channel tuple (second element), the tensor is considered to have an\n","          additional channel dimension. Thus, `dim=-1` for scalar-channel is equivalent to `dim=-2` for\n","          vector-channel, meaning the last spatial dimension before the channel dimension.\n","\n","    Example:\n","        >>> import torch\n","        >>> t1 = (torch.tensor([1, 2]), torch.tensor([[1, 2], [3, 4]]))\n","        >>> t2 = (torch.tensor([3, 4]), torch.tensor([[5, 6], [7, 8]]))\n","        >>> tuple_cat(t1, t2, dim=0)\n","        (tensor([1, 2, 3, 4]), tensor([[1, 2], [3, 4], [5, 6], [7, 8]]))\n","\n","        In this example, the scalar-channel tensors [1, 2] and [3, 4] are concatenated along dim=0,\n","        resulting in [1, 2, 3, 4]. The vector-channel tensors are 2x2 matrices concatenated along\n","        the first dimension (rows), resulting in a 4x2 matrix.\n","    '''\n","    dim %= len(args[0][0].shape)\n","    s_args, v_args = list(zip(*args))\n","    return torch.cat(s_args, dim=dim), torch.cat(v_args, dim=dim)\n","\n","\n","def tuple_index(x, idx):\n","    '''\n","    Indexes into a tuple (s, V) of two tensors along the first dimension.\n","\n","    This function allows indexing into each tensor of the tuple using the provided index.\n","    It is equivalent to applying the indexing operation on each tensor individually.\n","\n","    Parameters:\n","        x (tuple): A tuple of two tensors (s, V) where `s` is a scalar-channel tensor\n","                   and `V` is a vector-channel tensor.\n","        idx (int, slice, tensor, list): An index or a slice object or any other legitimate\n","                                        indexer used in PyTorch for indexing into tensors.\n","                                        This will be applied to each tensor in the tuple.\n","\n","    Returns:\n","        tuple: A tuple of two tensors, each being the result of indexing the corresponding\n","               tensor in `x` with `idx`.\n","\n","    Example:\n","        >>> import torch\n","        >>> s = torch.tensor([1, 2, 3])\n","        >>> V = torch.tensor([[1, 2], [3, 4], [5, 6]])\n","        >>> x = (s, V)\n","        >>> tuple_index(x, 1)\n","        (2, tensor([3, 4]))\n","\n","        In this example, indexing with `1` extracts the second element of the tensor `s` and\n","        the second row of the tensor `V`. Hence, the result is (2, tensor([3, 4])).\n","\n","    '''\n","    return x[0][idx], x[1][idx]\n","\n","\n","def randn(n, dims, device=\"cpu\"):\n","    '''\n","    Returns random tuples (s, V) drawn elementwise from a normal distribution.\n","\n","    This function generates two tensors. The first tensor `s` contains scalar values and the second tensor `V` contains vector values.\n","    Each element in these tensors is drawn from a standard normal distribution.\n","\n","    Parameters:\n","        n (int): The number of data points, which corresponds to the size of the first dimension in both tensors.\n","        dims (tuple): A tuple (n_scalar, n_vector) specifying the dimensions.\n","                      `n_scalar` is the number of scalar values per data point, and `n_vector` is the number of vector values per data point.\n","        device (str): The device to store the tensors on. Default is \"cpu\".\n","\n","    Returns:\n","        tuple: A tuple (s, V) where `s` is a tensor with shape (n, n_scalar) containing scalar values,\n","               and `V` is a tensor with shape (n, n_vector, 3) containing vector values.\n","\n","    Example:\n","        >>> randn(4, (2, 3))\n","        (tensor([[ 0.4963, -0.1383], [-0.2341,  1.5792], [ 0.7674, -0.4695], [ 0.5426, -0.4634]]),\n","         tensor([[[-1.5610, -0.3252,  1.0913],\n","                  [-1.3120,  0.0693,  0.1527],\n","                  [-0.1646, -0.4303,  0.7674]],\n","\n","                 [[-0.4695,  0.5426, -0.4634],\n","                  [ 0.4963, -0.1383,  0.4963],\n","                  [ 0.4963, -0.1383,  0.4963]],\n","\n","                 [[ 0.4963, -0.1383,  0.4963],\n","                  [ 0.4963, -0.1383,  0.4963],\n","                  [ 0.4963, -0.1383,  0.4963]],\n","\n","                 [[ 0.4963, -0.1383,  0.4963],\n","                  [ 0.4963, -0.1383,  0.4963],\n","                  [ 0.4963, -0.1383,  0.4963]]]))\n","\n","    In this example, calling `randn(4, (2, 3))` will generate a tuple of two tensors.\n","    The first tensor `s` will have the shape (4, 2) and the second tensor `V` will have the shape (4, 3, 3),\n","    with all elements drawn from a standard normal distribution.\n","    '''\n","    return torch.randn(n, dims[0], device=device), torch.randn(n, dims[1], 3, device=device)\n","\n","\n","def _norm_no_nan(x, axis=-1, keepdims=False, eps=1e-8, sqrt=True):\n","    '''\n","    Calculates the L2 norm of a tensor, clamped to a minimum value of `eps` to avoid division by zero or NaN values.\n","    Optionally, the square of the L2 norm can be returned.\n","\n","    Parameters:\n","        x (Tensor): The input tensor.\n","        axis (int): The dimension along which to compute the norm. Default is -1, which means the last dimension.\n","        keepdims (bool): If True, the output tensor will keep the dimension of size one along the specified axis;\n","                         otherwise, the dimension is squeezed. Default is False.\n","        eps (float): A small value to clamp the norm result to avoid NaN or infinite values. Default is 1e-8.\n","        sqrt (bool): If True, returns the square root of the sum of squares (L2 norm); if False, returns\n","                     the sum of squares without taking the square root. Default is True.\n","\n","    Returns:\n","        Tensor: The L2 norm of the tensor, clamped to a minimum value of `eps`.\n","\n","    Example:\n","        >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","        >>> _norm_no_nan(x, axis=1, sqrt=True)\n","        tensor([3.7417, 8.7749])\n","\n","        In this example, the function calculates the L2 norm along axis 1 for each row in the tensor `x`.\n","        The norms are [sqrt(1^2 + 2^2 + 3^2), sqrt(4^2 + 5^2 + 6^2)] = [3.7417, 8.7749], avoiding any NaN values\n","        by clamping the minimum norm value to `eps`.\n","    '''\n","    out = torch.clamp(torch.sum(torch.square(x), axis, keepdims), min=eps)\n","    return torch.sqrt(out) if sqrt else out\n","\n","\n","def _split(x, nv):\n","    '''\n","    Splits a merged tensor representation of a tuple (s, V) back into the tuple format. This function should\n","    be used to reverse the operation performed by `_merge(s, V)`. It's specifically designed for scenarios where\n","    working with a merged representation is necessary, and later the original tuple format is required.\n","\n","    Parameters:\n","        x (Tensor): A merged tensor representation of (s, V), where `V` vector channels are flattened and\n","                    appended to `s` scalar channels. This is the output from `_merge`.\n","        nv (int): The number of vector channels in `V` before the merge. This is used to correctly split\n","                  the tensor back into scalar and vector components.\n","\n","    Returns:\n","        tuple: A tuple (s, V), where `s` is the tensor of scalar channels, and `V` is the tensor of vector\n","               channels with the original shape.\n","\n","    Example:\n","        >>> s = torch.tensor([1, 2, 3])\n","        >>> V = torch.tensor([[4, 5, 6], [7, 8, 9]])\n","        >>> x = _merge(s, V)\n","        >>> _split(x, 2)\n","        (tensor([1, 2, 3]), tensor([[4, 5, 6], [7, 8, 9]]))\n","\n","        After merging `s` and `V` into `x`, `_split` correctly separates them back into the original `s` and `V` tensors.\n","    '''\n","    v = torch.reshape(x[..., -3*nv:], x.shape[:-1] + (nv, 3))\n","    s = x[..., :-3*nv]\n","    return s, v\n","\n","def _merge(s, v):\n","    '''\n","    Merges a tuple (s, V) into a single `torch.Tensor` by flattening the vector channels `V` and\n","    appending them to the scalar channels `s`. This function is useful for operations that require\n","    a unified tensor representation instead of separate scalar and vector components. To revert to the\n","    original tuple format, use `_split(x, nv)`.\n","\n","    Parameters:\n","        s (Tensor): The tensor of scalar channels.\n","        v (Tensor): The tensor of vector channels.\n","\n","    Returns:\n","        Tensor: A single tensor where the vector channels are flattened and concatenated to the scalar channels.\n","\n","    Example:\n","        >>> s = torch.tensor([1, 2, 3])\n","        >>> V = torch.tensor([[4, 5, 6], [7, 8, 9]])\n","        >>> _merge(s, V)\n","        tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n","\n","        The `V` tensor is flattened and appended to the `s` tensor to form a single concatenated tensor.\n","    '''\n","    v = torch.reshape(v, v.shape[:-2] + (3*v.shape[-2],))\n","    return torch.cat([s, v], -1)"]},{"cell_type":"markdown","metadata":{"id":"Yh3Z0-jAUDlJ"},"source":["## GVP\n","Vector gates only needed for atomic level tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjHkSDZ6T6py"},"outputs":[],"source":["class GVP(nn.Module):\n","    '''\n","    Geometric Vector Perceptron. See manuscript and README.md\n","    for more details.\n","\n","    :param in_dims: tuple (n_scalar, n_vector)\n","    :param out_dims: tuple (n_scalar, n_vector)\n","    :param h_dim: intermediate number of vector channels, optional\n","    :param activations: tuple of functions (scalar_act, vector_act)\n","    :param vector_gate: whether to use vector gating.\n","                        (vector_act will be used as sigma^+ in vector gating if `True`)\n","    '''\n","    def __init__(self, in_dims, out_dims, h_dim=None,\n","                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n","        super(GVP, self).__init__()\n","        self.si, self.vi = in_dims\n","        self.so, self.vo = out_dims\n","        self.vector_gate = vector_gate\n","        if self.vi:\n","            self.h_dim = h_dim or max(self.vi, self.vo)\n","            self.wh = nn.Linear(self.vi, self.h_dim, bias=False)\n","            self.ws = nn.Linear(self.h_dim + self.si, self.so)\n","            if self.vo:\n","                self.wv = nn.Linear(self.h_dim, self.vo, bias=False)\n","                if self.vector_gate: self.wsv = nn.Linear(self.so, self.vo)\n","        else:\n","            self.ws = nn.Linear(self.si, self.so)\n","\n","        self.scalar_act, self.vector_act = activations\n","        self.dummy_param = nn.Parameter(torch.empty(0))\n","\n","    def forward(self, x):\n","        '''\n","        :param x: tuple (s, V) of `torch.Tensor`,\n","                  or (if vectors_in is 0), a single `torch.Tensor`\n","        :return: tuple (s, V) of `torch.Tensor`,\n","                 or (if vectors_out is 0), a single `torch.Tensor`\n","        '''\n","        if self.vi:\n","            s, v = x\n","            v = torch.transpose(v, -1, -2)\n","            vh = self.wh(v)\n","            vn = _norm_no_nan(vh, axis=-2)\n","            s = self.ws(torch.cat([s, vn], -1))\n","            if self.vo:\n","                v = self.wv(vh)\n","                v = torch.transpose(v, -1, -2)\n","                if self.vector_gate:\n","                    if self.vector_act:\n","                        gate = self.wsv(self.vector_act(s))\n","                    else:\n","                        gate = self.wsv(s)\n","                    v = v * torch.sigmoid(gate).unsqueeze(-1)\n","                elif self.vector_act:\n","                    v = v * self.vector_act(\n","                        _norm_no_nan(v, axis=-1, keepdims=True))\n","        else:\n","            s = self.ws(x)\n","            if self.vo:\n","                v = torch.zeros(s.shape[0], self.vo, 3,\n","                                device=self.dummy_param.device)\n","        if self.scalar_act:\n","            s = self.scalar_act(s)\n","\n","        return (s, v) if self.vo else s"]},{"cell_type":"markdown","metadata":{"id":"fd3BGp6AUKiA"},"source":["## Vector Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8hwHzszUGFh"},"outputs":[],"source":["class _VDropout(nn.Module):\n","    '''\n","    Vector channel dropout where the elements of each\n","    vector channel are dropped together.\n","    '''\n","    def __init__(self, drop_rate):\n","        super(_VDropout, self).__init__()\n","        self.drop_rate = drop_rate\n","        self.dummy_param = nn.Parameter(torch.empty(0))\n","\n","    def forward(self, x):\n","        '''\n","        :param x: `torch.Tensor` corresponding to vector channels\n","        '''\n","        device = self.dummy_param.device\n","        if not self.training:\n","            return x\n","        mask = torch.bernoulli(\n","            (1 - self.drop_rate) * torch.ones(x.shape[:-1], device=device)\n","        ).unsqueeze(-1)\n","        x = mask * x / (1 - self.drop_rate)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"FXFPmONyUM9U"},"source":["## Combined scalar-vector tuple dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLOrWtXrURkb"},"outputs":[],"source":["class Dropout(nn.Module):\n","    '''\n","    Combined dropout for tuples (s, V).\n","    Takes tuples (s, V) as input and as output.\n","    '''\n","    def __init__(self, drop_rate):\n","        super(Dropout, self).__init__()\n","        self.sdropout = nn.Dropout(drop_rate)\n","        self.vdropout = _VDropout(drop_rate)\n","\n","    def forward(self, x):\n","        '''\n","        :param x: tuple (s, V) of `torch.Tensor`,\n","                  or single `torch.Tensor`\n","                  (will be assumed to be scalar channels)\n","        '''\n","        if type(x) is torch.Tensor:\n","            return self.sdropout(x)\n","        s, v = x\n","        return self.sdropout(s), self.vdropout(v)"]},{"cell_type":"markdown","metadata":{"id":"58KzaQ1xUZzh"},"source":["## Layer Normalization for scalar-vector tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yat7BsxGUbp8"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","    '''\n","    Combined LayerNorm for tuples (s, V).\n","    Takes tuples (s, V) as input and as output.\n","    '''\n","    def __init__(self, dims):\n","        super(LayerNorm, self).__init__()\n","        self.s, self.v = dims\n","        self.scalar_norm = nn.LayerNorm(self.s)\n","\n","    def forward(self, x):\n","        '''\n","        :param x: tuple (s, V) of `torch.Tensor`,\n","                  or single `torch.Tensor`\n","                  (will be assumed to be scalar channels)\n","        '''\n","        if not self.v:\n","            return self.scalar_norm(x)\n","        s, v = x\n","        vn = _norm_no_nan(v, axis=-1, keepdims=True, sqrt=False)\n","        vn = torch.sqrt(torch.mean(vn, dim=-2, keepdim=True))\n","        return self.scalar_norm(s), v / vn"]},{"cell_type":"markdown","metadata":{"id":"FLrVEMvaUjrA"},"source":["## Graph Convolution Step"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBjQ3l8EUziS"},"outputs":[],"source":["class GVPConv(MessagePassing):\n","    '''\n","    Graph convolution / message passing with Geometric Vector Perceptrons.\n","    Takes in a graph with node and edge embeddings,\n","    and returns new node embeddings.\n","\n","    This does NOT do residual updates and pointwise feedforward layers\n","    ---see `GVPConvLayer`.\n","\n","    :param in_dims: input node embedding dimensions (n_scalar, n_vector)\n","    :param out_dims: output node embedding dimensions (n_scalar, n_vector)\n","    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n","    :param n_layers: number of GVPs in the message function\n","    :param module_list: preconstructed message function, overrides n_layers\n","    :param aggr: should be \"add\" if some incoming edges are masked, as in\n","                 a masked autoregressive decoder architecture, otherwise \"mean\"\n","    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n","    :param vector_gate: whether to use vector gating.\n","                        (vector_act will be used as sigma^+ in vector gating if `True`)\n","    '''\n","    def __init__(self, in_dims, out_dims, edge_dims,\n","                 n_layers=3, module_list=None, aggr=\"mean\",\n","                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n","        super(GVPConv, self).__init__(aggr=aggr)\n","        self.si, self.vi = in_dims\n","        self.so, self.vo = out_dims\n","        self.se, self.ve = edge_dims\n","\n","        GVP_ = functools.partial(GVP,\n","                activations=activations, vector_gate=vector_gate)\n","\n","        module_list = module_list or []\n","        if not module_list:\n","            if n_layers == 1:\n","                module_list.append(\n","                    GVP_((2*self.si + self.se, 2*self.vi + self.ve),\n","                        (self.so, self.vo), activations=(None, None)))\n","            else:\n","                module_list.append(\n","                    GVP_((2*self.si + self.se, 2*self.vi + self.ve), out_dims)\n","                )\n","                for i in range(n_layers - 2):\n","                    module_list.append(GVP_(out_dims, out_dims))\n","                module_list.append(GVP_(out_dims, out_dims,\n","                                       activations=(None, None)))\n","        self.message_func = nn.Sequential(*module_list)\n","\n","    def forward(self, x, edge_index, edge_attr):\n","        '''\n","        :param x: tuple (s, V) of `torch.Tensor`\n","        :param edge_index: array of shape [2, n_edges]\n","        :param edge_attr: tuple (s, V) of `torch.Tensor`\n","        The self.propagate method is directky inherited from the MessagePassing\n","        class and it calls upon the message method, which has been overridden in this case.\n","        '''\n","        x_s, x_v = x\n","        message = self.propagate(edge_index,\n","                    s=x_s, v=x_v.reshape(x_v.shape[0], 3*x_v.shape[1]),\n","                    edge_attr=edge_attr)\n","        return _split(message, self.vo)\n","\n","    def message(self, s_i, v_i, s_j, v_j, edge_attr):\n","        v_j = v_j.view(v_j.shape[0], v_j.shape[1]//3, 3)\n","        v_i = v_i.view(v_i.shape[0], v_i.shape[1]//3, 3)\n","        message = tuple_cat((s_j, v_j), edge_attr, (s_i, v_i))\n","        message = self.message_func(message)\n","        return _merge(*message)"]},{"cell_type":"markdown","metadata":{"id":"qyoM8Nne5d0T"},"source":["## GCN Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDaxY7Vf5jQL"},"outputs":[],"source":["class GVPConvLayer(nn.Module):\n","    '''\n","    Full graph convolution / message passing layer with\n","    Geometric Vector Perceptrons. Residually updates node embeddings with\n","    aggregated incoming messages, applies a pointwise feedforward\n","    network to node embeddings, and returns updated node embeddings.\n","\n","    To only compute the aggregated messages, see `GVPConv`.\n","\n","    :param node_dims: node embedding dimensions (n_scalar, n_vector)\n","    :param edge_dims: input edge embedding dimensions (n_scalar, n_vector)\n","    :param n_message: number of GVPs to use in message function\n","    :param n_feedforward: number of GVPs to use in feedforward function\n","    :param drop_rate: drop probability in all dropout layers\n","    :param autoregressive: if `True`, this `GVPConvLayer` will be used\n","           with a different set of input node embeddings for messages\n","           where src >= dst\n","    :param activations: tuple of functions (scalar_act, vector_act) to use in GVPs\n","    :param vector_gate: whether to use vector gating.\n","                        (vector_act will be used as sigma^+ in vector gating if `True`)\n","    '''\n","    def __init__(self, node_dims, edge_dims,\n","                 n_message=3, n_feedforward=2, drop_rate=.1,\n","                 autoregressive=False,\n","                 activations=(F.relu, torch.sigmoid), vector_gate=False):\n","\n","        super(GVPConvLayer, self).__init__()\n","        self.conv = GVPConv(node_dims, node_dims, edge_dims, n_message,\n","                           aggr=\"add\" if autoregressive else \"mean\",\n","                           activations=activations, vector_gate=vector_gate)\n","        GVP_ = functools.partial(GVP,\n","                activations=activations, vector_gate=vector_gate)\n","        self.norm = nn.ModuleList([LayerNorm(node_dims) for _ in range(2)])\n","        self.dropout = nn.ModuleList([Dropout(drop_rate) for _ in range(2)])\n","\n","        ff_func = []\n","        if n_feedforward == 1:\n","            ff_func.append(GVP_(node_dims, node_dims, activations=(None, None)))\n","        else:\n","            hid_dims = 4*node_dims[0], 2*node_dims[1]\n","            ff_func.append(GVP_(node_dims, hid_dims))\n","            for i in range(n_feedforward-2):\n","                ff_func.append(GVP_(hid_dims, hid_dims))\n","            ff_func.append(GVP_(hid_dims, node_dims, activations=(None, None)))\n","        self.ff_func = nn.Sequential(*ff_func)\n","\n","    def forward(self, x, edge_index, edge_attr,\n","                autoregressive_x=None, node_mask=None):\n","        '''\n","        :param x: tuple (s, V) of `torch.Tensor`\n","        :param edge_index: array of shape [2, n_edges]\n","        :param edge_attr: tuple (s, V) of `torch.Tensor`\n","        :param autoregressive_x: tuple (s, V) of `torch.Tensor`.\n","                If not `None`, will be used as src node embeddings\n","                for forming messages where src >= dst. The corrent node\n","                embeddings `x` will still be the base of the update and the\n","                pointwise feedforward.\n","        :param node_mask: array of type `bool` to index into the first\n","                dim of node embeddings (s, V). If not `None`, only\n","                these nodes will be updated.\n","        '''\n","\n","        if autoregressive_x is not None:\n","            src, dst = edge_index\n","            mask = src < dst\n","            edge_index_forward = edge_index[:, mask]\n","            edge_index_backward = edge_index[:, ~mask]\n","            edge_attr_forward = tuple_index(edge_attr, mask)\n","            edge_attr_backward = tuple_index(edge_attr, ~mask)\n","\n","            dh = tuple_sum(\n","                self.conv(x, edge_index_forward, edge_attr_forward),\n","                self.conv(autoregressive_x, edge_index_backward, edge_attr_backward)\n","            )\n","\n","            count = scatter_add(torch.ones_like(dst), dst,\n","                        dim_size=dh[0].size(0)).clamp(min=1).unsqueeze(-1)\n","\n","            dh = dh[0] / count, dh[1] / count.unsqueeze(-1)\n","\n","        else:\n","            dh = self.conv(x, edge_index, edge_attr)\n","\n","        if node_mask is not None:\n","            x_ = x\n","            x, dh = tuple_index(x, node_mask), tuple_index(dh, node_mask)\n","\n","        x = self.norm[0](tuple_sum(x, self.dropout[0](dh)))\n","\n","        dh = self.ff_func(x)\n","        x = self.norm[1](tuple_sum(x, self.dropout[1](dh)))\n","\n","        if node_mask is not None:\n","            x_[0][node_mask], x_[1][node_mask] = x[0], x[1]\n","            x = x_\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"wommx02TU9dH"},"source":["## Final Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kK7jUbUuU9C_"},"outputs":[],"source":["class ProteinGVPModel(nn.Module):\n","    '''\n","    GVP-GNN for Regression\n","\n","    Should be used with `gvp.data.ProteinGraphDataset`, or with generators\n","    of `torch_geometric.data.Batch` objects with the same attributes.\n","\n","    :param node_in_dim: node dimensions in input graph, should be\n","                        (6, 3) if using original features,\n","                        (emb_dim+6, 3) if using ESM/PLM embeddings\n","    :param node_h_dim: node dimensions to use in GVP-GNN layers\n","    :param edge_in_dim: edge dimensions in input graph, should be\n","                        (32, 1) if using original features\n","    :param edge_h_dim: edge dimensions to embed to before use\n","                       in GVP-GNN layers\n","    :param num_layers: number of GVP-GNN layers\n","    :param drop_rate: rate to use in all dropout layers\n","    '''\n","    def __init__(self, node_in_dim, node_h_dim,\n","                 edge_in_dim, edge_h_dim,\n","                 num_layers=3, drop_rate=0.1):\n","\n","        super(ProteinGVPModel, self).__init__()\n","\n","        self.W_v = nn.Sequential(\n","            LayerNorm(node_in_dim),\n","            GVP(node_in_dim, node_h_dim, activations=(None, None))\n","        )\n","        self.W_e = nn.Sequential(\n","            LayerNorm(edge_in_dim),\n","            GVP(edge_in_dim, edge_h_dim, activations=(None, None))\n","        )\n","\n","        # can play around with some dimensions here, when using ESM/PLM\n","        # embeddings\n","        self.layers = nn.ModuleList(\n","                GVPConvLayer(node_h_dim, edge_h_dim, drop_rate=drop_rate)\n","            for _ in range(num_layers))\n","\n","        ns, _ = node_h_dim\n","        self.W_out = nn.Sequential(\n","            LayerNorm(node_h_dim),\n","            GVP(node_h_dim, (ns, 0)))\n","\n","        self.dense = nn.Sequential(\n","            nn.Linear(ns, 2*ns), nn.ReLU(inplace=True),\n","            nn.Dropout(p=drop_rate),\n","            nn.Linear(2*ns, 1)\n","        )\n","\n","    def forward(self, h_V, edge_index, h_E, batch=None):\n","        '''\n","        :param h_V: tuple (s, V) of node embeddings\n","        :param edge_index: `torch.Tensor` of shape [2, num_edges]\n","        :param h_E: tuple (s, V) of edge embeddings\n","        Very important to specify batch, otherwise you get one output\n","        for each batch, instead of one output for each batch in the graph\n","        '''\n","        h_V = self.W_v(h_V)\n","        h_E = self.W_e(h_E)\n","        for layer in self.layers:\n","            h_V = layer(h_V, edge_index, h_E)\n","        out = self.W_out(h_V)\n","\n","        if batch is None: out = out.mean(dim=0, keepdims=True)\n","        else: out = scatter_mean(out, batch, dim=0)\n","        return self.dense(out).squeeze(-1)"]},{"cell_type":"markdown","metadata":{"id":"OwqAi14paUaZ"},"source":["##Train, eval, and plot functions"]},{"cell_type":"code","source":["def save_plot(y_pred, y_real, epoch, folder_path, file_name, fig_size=(12, 10), marker_size=10, fit_line_color=\"skyblue\", distn_color_1=\"lightgreen\", distn_color_2=\"salmon\"):\n","    sns.set(style=\"whitegrid\")\n","\n","    # Create a jointplot\n","    g = sns.jointplot(\n","        x=y_real,\n","        y=y_pred,\n","        kind=\"reg\",\n","        height=fig_size[1] - 1,\n","        color=fit_line_color,\n","        scatter_kws={\"s\": marker_size},\n","        marginal_kws={'color': distn_color_1}\n","    )\n","\n","    # Set axis labels\n","    g.ax_joint.set_xlabel(\"Actual Values\", fontsize=12)\n","    g.ax_joint.set_ylabel(\"Predictions\", fontsize=12)\n","\n","    # Set the title with a bit more space at the top\n","    g.fig.suptitle(f\"Predictions vs. Actual Values\\n R = {np.round(stats.pearsonr(y_pred, y_real)[0], 3)} | Epoch: {epoch}\", fontsize=14, y=1.05)\n","\n","    # Plot the histograms\n","    sns.histplot(y_real, color=distn_color_1, alpha=0.6, ax=g.ax_marg_x, fill=True, kde=True)\n","    sns.histplot(y=y_pred, color=distn_color_2, alpha=0.6, ax=g.ax_marg_y, fill=True, kde=True)\n","\n","    # Adjust the plot margins and layout\n","    plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)\n","    g.fig.tight_layout()\n","\n","    # Save the plot\n","    plt.savefig(Path(folder_path) / file_name, bbox_inches='tight')\n","    plt.show()\n","    plt.close(g.fig)"],"metadata":{"id":"UISP0ATIAt8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibBILMmfaVqe"},"outputs":[],"source":["def train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, epochs, device, save_path):\n","    best_r_score = float('-inf')\n","    save_model_path = save_path+'/model_checkpoints'\n","    os.makedirs(os.path.dirname(save_model_path), exist_ok=True)\n","    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n","    for epoch in range(epochs):\n","        model.train()\n","        total_loss = 0\n","        for data in train_loader:\n","            optimizer.zero_grad()\n","            node_embeddings = (data.node_s.to(device), data.node_v.to(device))  # Include vector parts\n","            edge_embeddings = (data.edge_s.to(device), data.edge_v.to(device))  # Include vector parts\n","            # never forget to add data.batch.to(device) as a parameter\n","            outputs = model(node_embeddings, data.edge_index.to(device), edge_embeddings, data.batch.to(device))\n","            loss = criterion(outputs, data.target.to(device))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n","        if (epoch + 1) % 5 == 0:\n","            torch.save(model.state_dict(), Path(save_model_path) / f\"model_epoch_{epoch+1}.pt\")\n","\n","        ## Uncomment the following to check for improved R-score every 10 epochs\n","        ## and saving the plot. Ideally we'll train for 50+ epochs\n","        # if (epoch + 1) % 10 == 0:\n","        #     output_dict = test_model(model, test_loader, device)\n","        #     r_score = output_dict[\"r_value\"]\n","        #     predictions = output_dict[\"preds\"]\n","        #     actuals = output_dict[\"targets\"]\n","        #     if r_score > best_r_score:\n","        #         best_r_score = r_score\n","        #         torch.save(model.state_dict(), Path(save_path) / f\"best_model_epoch_{epoch+1}.pt\")\n","        #         print(f\"New best model saved with R={r_score:.4f} at epoch {epoch+1}\")\n","        #         save_plot(np.array(predictions), np.array(actuals), epoch + 1, save_path, f\"plot_epoch_{epoch+1}.png\")\n","\n","    return best_r_score\n","\n","def test_model(model, test_loader, device):\n","    model.eval()\n","    predictions, actuals = [], []\n","    with torch.no_grad():\n","        for data in test_loader:\n","            node_embeddings = (data.node_s.to(device), data.node_v.to(device))  # Include vector parts\n","            edge_embeddings = (data.edge_s.to(device), data.edge_v.to(device))  # Include vector parts\n","            # never forget to add data.batch.to(device) as a parameter\n","            outputs = model(node_embeddings, data.edge_index.to(device), edge_embeddings, data.batch.to(device))\n","            predictions.extend(outputs.cpu().numpy())\n","            actuals.extend(data.target.cpu().numpy())\n","\n","    r_value, _ = pearsonr(predictions, actuals)\n","    mae = mean_absolute_error(actuals, predictions)\n","    mse = mean_squared_error(actuals, predictions)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(actuals, predictions)\n","    spearman_rho, _ = spearmanr(actuals, predictions)\n","\n","    print(f'Test R-value: {r_value:.4f}')\n","    print(f'Test MAE: {mae:.4f}')\n","    print(f'Test MSE: {mse:.4f}')\n","    print(f'Test RMSE: {rmse:.4f}')\n","    print(f'Test R2: {r2:.4f}')\n","    print(f'Test Spearman rho: {spearman_rho:.4f}')\n","    return {\n","        \"r_value\": r_value,\n","        \"mae\": mae,\n","        \"mse\": mse,\n","        \"rmse\": rmse,\n","        \"r2\": r2,\n","        \"spearman_rho\": spearman_rho,\n","        \"preds\": predictions,\n","        \"targets\": actuals\n","    }"]},{"cell_type":"markdown","source":["## Importing data, initializing batch_samplers, and creating dataloaders"],"metadata":{"id":"jfE-4e3neNm7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yBf9-A5waxe7"},"outputs":[],"source":["pdb_folder_path = '/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/omegafold_predicted_structures/'\n","embeddings_path = '/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/emb_residue_level_embedding_GB1_embedding_ESM_2_650_embeddings_tensor.pt'\n","split_fasta_file_path = '/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/three_vs_rest.fasta'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3UGhxDxa0Dl"},"outputs":[],"source":["split_name = os.path.splitext(os.path.basename(split_fasta_file_path))[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eWYtEpQaxfH"},"outputs":[],"source":["use_embeddings = True\n","emb_model_list = ['ESM_2_650M', 'ESM_2_3B', 'ESM_2_15B']\n","emb_model = emb_model_list[0]\n","if use_embeddings:\n","    embeddings = torch.load(embeddings_path, map_location=device)\n","    embeddings_use_str = '/with_embeddings'\n","    if emb_model == 'ESM_2_650M':\n","        emb_dim = 1280\n","    elif emb_model == 'ESM_2_3B':\n","        emb_dim = 2560\n","    elif emb_model == 'ESM_2_15B':\n","        emb_dim = 5120\n","else:\n","    embeddings = None\n","    embeddings_use_str = '/without_embeddings'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlNPBBL5axfH"},"outputs":[],"source":["data_info = parse_fasta(split_fasta_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GO41OL8axfH"},"outputs":[],"source":["pdb_files = [pdb_folder_path + f'Sequence{i}.pdb' for i in range(8733)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tAzkJxVAaxfI"},"outputs":[],"source":["train_dataset= ProteinGraphDataset(pdb_files, data_info, 'train', use_embeddings=use_embeddings, embeddings = embeddings, device = device)\n","test_dataset= ProteinGraphDataset(pdb_files, data_info, 'test', use_embeddings=use_embeddings, embeddings = embeddings, device = device)"]},{"cell_type":"markdown","source":["You have to uncomment the node_counts_train and node_counts_test lines to calculate it automatcially. Otherwise you can pre-compute it and save it and access as shown."],"metadata":{"id":"Tlt5fffceXsX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAeCv7Oslrww"},"outputs":[],"source":["# node_counts_train = [len(train_dataset[i].x) for i in range(len(train_dataset))]\n","nodes_train_save_path = f'/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/node_counts_train_{split_name}.pkl'\n","os.makedirs(os.path.dirname(nodes_train_save_path), exist_ok=True)\n","\n","## Save node counts to Google Drive\n","# with open(nodes_train_save_path , 'wb') as f:\n","#     pickle.dump(node_counts_train, f)\n","\n","## Use the following if pre-computed the number of nodes for trainset\n","with open(nodes_train_save_path, 'rb') as f:\n","    node_counts_train = pickle.load(f)\n","\n","# node_counts_test = [len(test_dataset[i].x) for i in range(len(test_dataset))]\n","nodes_test_save_path = f'/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/node_counts_test_{split_name}.pkl'\n","os.makedirs(os.path.dirname(nodes_test_save_path), exist_ok=True)\n","\n","## Use the following if pre-computed the number of nodes for testset\n","## Save node counts to Google Drive\n","# with open(nodes_test_save_path , 'wb') as f:\n","#     pickle.dump(node_counts_test, f)\n","\n","with open(nodes_test_save_path, 'rb') as f:\n","    node_counts_test = pickle.load(f)"]},{"cell_type":"code","source":["batch_sampler_train = BatchSampler(node_counts_train)\n","batch_sampler_test = BatchSampler(node_counts_test)"],"metadata":{"id":"0n32prSDen0K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initializing Dataloaders"],"metadata":{"id":"aCefvxC1fDdA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UF6KC7YPaxfI"},"outputs":[],"source":["# DataLoader for training\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=batch_sampler_train, collate_fn=custom_collate)\n","# DataLoader for testing\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_sampler=batch_sampler_test, collate_fn=custom_collate)"]},{"cell_type":"markdown","source":["### Initializing the model based on whether or not the ESM/PLM embeddings are to be used"],"metadata":{"id":"q7RDBIrafM3B"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HKumy3AuOnF"},"outputs":[],"source":["if not use_embeddings:\n","    model = ProteinGVPModel(node_in_dim=(6, 3), node_h_dim=(100, 16), edge_in_dim=(32, 1), edge_h_dim=(32, 1), num_layers=3, drop_rate=0.1).to(device)\n","else:\n","    model = ProteinGVPModel(node_in_dim=(emb_dim+6, 3), node_h_dim=(100, 16), edge_in_dim=(32, 1), edge_h_dim=(32, 1), num_layers=3, drop_rate=0.1).to(device)\n","optimizer = Adam(model.parameters(), lr=0.001)\n","criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6xFhmyDt9Xm"},"outputs":[],"source":["epochs = 10\n","save_path = \"/content/drive/MyDrive/function_predictor/GB1-Dataset-FewToMore/gnn_results/\" + split_name + embeddings_use_str"]},{"cell_type":"markdown","source":["### Train and evaluate"],"metadata":{"id":"SoABlRmnfWCw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rl6lNfbGZGA_"},"outputs":[],"source":["best_r_score = train_and_evaluate(model, train_loader, test_loader, optimizer, criterion, epochs, device, save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2gW0-CyZZK2T"},"outputs":[],"source":["model_save_path = save_path+'/model_checkpoints/model_epoch_10.pt'\n","model.load_state_dict(torch.load(Path(model_save_path)))\n","output_dict1 = test_model(model, test_loader, device)"]},{"cell_type":"code","source":["r_score = output_dict1['r_value']\n","print(f\"Final best R-score: {r_score}\")"],"metadata":{"id":"BcUviUOZQfYd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds = output_dict1['preds']\n","targets = output_dict1['targets']\n","save_plot(np.array(preds), np.array(targets), 10, save_path, f\"plot_epoch_{10}.png\")"],"metadata":{"id":"Q3hBsvqdBUP2"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPQnSebTBbmdMBaBzMmaKgU"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}