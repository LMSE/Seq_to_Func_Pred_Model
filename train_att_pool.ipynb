{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOgFQQyHwAQHdVF2u5AwLGV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"un4AJzKlD92F","executionInfo":{"status":"ok","timestamp":1706403627163,"user_tz":-330,"elapsed":8159,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"outputs":[],"source":["import os\n","import sys\n","import os.path\n","from sys import platform\n","from pathlib import Path\n","import re\n","import sys\n","import time\n","import copy\n","import math\n","import scipy\n","import torch\n","import pickle\n","import random\n","import argparse\n","import subprocess\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","from torch.utils import data\n","from torch.nn.utils.weight_norm import weight_norm\n","from sklearn import datasets\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.metrics import auc\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n","import seaborn as sns\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from copy import deepcopy\n","from ipywidgets import IntProgress\n","from datetime import datetime\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"F7mFiqY7ETuc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706403652187,"user_tz":-330,"elapsed":25032,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"f00b0a06-d1c5-4ed3-e932-5b9b542aa428"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["# import sys\n","# sys.path.append('/content/gdrive/MyDrive/function_predictor/code/')"],"metadata":{"id":"x61hl34tEUJ6","executionInfo":{"status":"ok","timestamp":1706403652187,"user_tz":-330,"elapsed":6,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"fxQSvBX3WrG_","executionInfo":{"status":"ok","timestamp":1706403652187,"user_tz":-330,"elapsed":6,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class StandardScaler:\n","    \"\"\"A :class:`StandardScaler` normalizes the features of a dataset.\n","\n","    When it is fit on a dataset, the :class:`StandardScaler` learns the mean and standard deviation across the 0th axis.\n","    When transforming a dataset, the :class:`StandardScaler` subtracts the means and divides by the standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, means: np.ndarray = None, stds: np.ndarray = None, replace_nan_token: Any = None):\n","        \"\"\"\n","        :param means: An optional 1D numpy array of precomputed means.\n","        :param stds: An optional 1D numpy array of precomputed standard deviations.\n","        :param replace_nan_token: A token to use to replace NaN entries in the features.\n","        \"\"\"\n","        self.means = means\n","        self.stds = stds\n","        self.replace_nan_token = replace_nan_token\n","\n","    def fit(self, X: List[List[Optional[float]]]) -> 'StandardScaler':\n","        \"\"\"\n","        Learns means and standard deviations across the 0th axis of the data :code:`X`.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The fitted :class:`StandardScaler` (self).\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        self.means = np.nanmean(X, axis=0)\n","        self.stds = np.nanstd(X, axis=0)\n","        self.means = np.where(np.isnan(self.means), np.zeros(self.means.shape), self.means)\n","        self.stds = np.where(np.isnan(self.stds), np.ones(self.stds.shape), self.stds)\n","        self.stds = np.where(self.stds == 0, np.ones(self.stds.shape), self.stds)\n","\n","        return self\n","\n","    def transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Transforms the data by subtracting the means and dividing by the standard deviations.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = (X - self.means) / self.stds\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none.astype(np.float32)\n","\n","    def inverse_transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Performs the inverse transformation by multiplying by the standard deviations and adding the means.\n","\n","        :param X: A list of lists of floats.\n","        :return: The inverse transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = X * self.stds + self.means\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none.astype('float32')\n","\n","\n","def normalize_targets(y_data) -> StandardScaler:\n","    # For Future Use.\n","    \"\"\"\n","    Normalizes the targets of the dataset using a :class:`~chemprop.data.StandardScaler`.\n","\n","    The :class:`~chemprop.data.StandardScaler` subtracts the mean and divides by the standard deviation\n","    for each task independently.\n","\n","    This should only be used for regression datasets.\n","\n","    :return: A :class:`~chemprop.data.StandardScaler` fitted to the targets.\n","    \"\"\"\n","    scaler = StandardScaler().fit(y_data)\n","    scaled_targets = scaler.transform(y_data).tolist()\n","\n","    return scaled_targets, scaler"],"metadata":{"id":"KvNDGOoxblt1","executionInfo":{"status":"ok","timestamp":1706403652187,"user_tz":-330,"elapsed":5,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["seeds = [0,1,2,42,1234]\n","seed=seeds[1]\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"R5iON4nZEZET","executionInfo":{"status":"ok","timestamp":1706403652187,"user_tz":-330,"elapsed":5,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["root_path = '/content/gdrive/MyDrive/function_predictor/GB1-Dataset-FewToMore'"],"metadata":{"id":"LXsSkRmJIByS","executionInfo":{"status":"ok","timestamp":1706403652188,"user_tz":-330,"elapsed":6,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["embedding_file = root_path+'/'+'emb_residue_level_embedding_GB1_embedding_ESM_2_650_embeddings_tensor.pt'\n","embeddings = torch.load(embedding_file)"],"metadata":{"id":"ma9yTAMDqgQm","executionInfo":{"status":"ok","timestamp":1706403764056,"user_tz":-330,"elapsed":111873,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["splits = ['low_vs_high', 'one_vs_rest', 'two_vs_rest', 'three_vs_rest']\n","split_name = splits[-1]\n","fasta_file = root_path+'/'+split_name+'.fasta'"],"metadata":{"id":"pUtWyEmfBD_y","executionInfo":{"status":"ok","timestamp":1706403764057,"user_tz":-330,"elapsed":17,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# [New] Parse FASTA file and create datasets\n","sequences = {}\n","with open(fasta_file, 'r') as file:\n","    for line in file:\n","        if line.startswith('>'):\n","            name, target, set_info, _ = line.strip().split(' ')\n","            name = name[1:]\n","            target = float(target.split('=')[1])\n","            set_type = set_info.split('=')[1]\n","            sequences[name] = {'target': target, 'set': set_type}"],"metadata":{"id":"jfibCggIhFBe","executionInfo":{"status":"ok","timestamp":1706403765020,"user_tz":-330,"elapsed":979,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["train_data = []\n","test_data = []\n","for name, info in sequences.items():\n","    embedding = embeddings[f'{name}'].numpy()\n","    if info['set'] == 'train':\n","        train_data.append((embedding, info['target']))\n","    elif info['set'] == 'test':\n","        test_data.append((embedding, info['target']))"],"metadata":{"id":"o5DVV5IBLqR9","executionInfo":{"status":"ok","timestamp":1706403765021,"user_tz":-330,"elapsed":5,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n","\n","class ATT_dataset(Dataset):\n","    def __init__(self, data, max_len, X_scaler=None):\n","        super().__init__()\n","        self.data = data\n","        self.max_len = max_len\n","        self.X_scaler = X_scaler\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        embedding, target = self.data[idx]\n","        seq_len = len(embedding)\n","\n","        # Pad on-the-fly\n","        padded_embedding = np.zeros((self.max_len, embedding.shape[1]), dtype=np.float32)\n","        padded_embedding[:seq_len, :] = embedding\n","\n","        # Generate mask\n","        mask = np.zeros(self.max_len, dtype=np.float32)\n","        mask[:seq_len] = 1\n","\n","        return {'embedding': padded_embedding, 'mask': mask, 'target': np.array(target, dtype=np.float32)}\n","\n","    def collate_fn(self, batch):\n","        embeddings = [item['embedding'] for item in batch]\n","        masks = [item['mask'] for item in batch]\n","        targets = [item['target'] for item in batch]\n","\n","        # Convert to numpy arrays\n","        embeddings_array = np.stack(embeddings)\n","        masks_array = np.stack(masks)\n","        targets_array = np.stack(targets)\n","\n","        # Reshape for scaling: (batch_size * max_len, emb_dim)\n","        flat_embeddings = embeddings_array.reshape(-1, embeddings_array.shape[-1])\n","        if self.X_scaler is not None:\n","            scaled_flat_embeddings = self.X_scaler.transform(flat_embeddings)\n","        else:\n","            scaled_flat_embeddings = flat_embeddings\n","        scaled_embeddings = scaled_flat_embeddings.reshape(embeddings_array.shape)\n","\n","        # Convert to tensors\n","        embeddings_tensor = torch.from_numpy(scaled_embeddings)\n","        masks_tensor = torch.from_numpy(masks_array)\n","        targets_tensor = torch.from_numpy(targets_array)\n","\n","        return {\n","            'embeddings': embeddings_tensor,\n","            'masks': masks_tensor,\n","            'targets': targets_tensor\n","        }\n","\n","def generate_ATT_loader(train_data, test_data, max_len, batch_size, scaler: Optional[StandardScaler] = None):\n","    # Flatten all embeddings from train data for scaler fitting\n","    flat_train_embeddings = np.concatenate([item[0].reshape(-1, 1280) for item in train_data], axis=0)\n","\n","    if scaler is None:\n","        scaler = SklearnStandardScaler()\n","    scaler.fit(flat_train_embeddings)\n","\n","    train_dataset = ATT_dataset(train_data, max_len, scaler)\n","    test_dataset = ATT_dataset(test_data, max_len, scaler)\n","\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n","    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"frk_3apZkzWY","executionInfo":{"status":"ok","timestamp":1706403765021,"user_tz":-330,"elapsed":4,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","max_len = 265  # Set your sequence max length here\n","scaler = StandardScaler()  # Initialize your custom scaler\n","train_loader, test_loader = generate_ATT_loader(train_data, test_data, max_len, batch_size=256, scaler=scaler)"],"metadata":{"id":"EFeiFZzlk3mB","executionInfo":{"status":"ok","timestamp":1706403785678,"user_tz":-330,"elapsed":20661,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class ScaledDotProductAttention(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, Q, K, V, attn_mask):\n","        scores = torch.matmul(Q, K.transpose(-1,-2))\n","        scores.masked_fill_(attn_mask,-1e9)\n","        attn = nn.Softmax(dim=-1)(scores)\n","        context = torch.matmul(attn, V) # [batch_size, n_heads, len_q, d_v]\n","        return context, attn\n","\n","#====================================================================================================#\n","class MultiHeadAttentionwithonekey(nn.Module):\n","    def __init__(self,d_model,d_k,n_heads,d_v,out_dim):\n","        super().__init__()\n","        self.n_heads = n_heads\n","        self.d_k = d_k\n","        self.d_v = d_v\n","        self.out_dim = out_dim\n","        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n","        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n","        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n","        self.fc_att = nn.Linear(n_heads * d_v, out_dim, bias=False)\n","\n","    def forward(self, input_Q, input_K, input_V, attn_mask):\n","        Q = self.W_Q(input_Q).view(input_Q.size(0),-1, self.n_heads, self.d_k).transpose(1, 2)\n","        K = self.W_K(input_K).view(input_Q.size(0),-1, self.n_heads, self.d_k).transpose(1, 2)\n","        V = self.W_V(input_V).view(input_V.size(0),-1, self.n_heads, self.d_k).transpose(1, 2)\n","        #print(Q.size(), K.size())\n","        attn_mask = attn_mask.unsqueeze(1).repeat(1,self.n_heads,1,1)\n","        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\n","        context = context.transpose(1, 2).reshape(input_Q.size(0), -1, self.n_heads * self.d_v)\n","        output = self.fc_att(context) # [batch_size, len_q, out_dim]\n","        return output, attn\n","\n","#====================================================================================================#\n","class PoswiseFeedForwardNet(nn.Module):\n","    def __init__(self, d_model, d_ff):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(d_model, d_ff),\n","            nn.ReLU(),\n","            nn.Linear(d_ff, 1)\n","            )\n","        #--------------------------------------------------#\n","        self.weights = nn.Parameter(torch.from_numpy(np.array([0.0,])), requires_grad = True)\n","        self.fc_1    = nn.Linear(d_model, d_ff)\n","        self.fc_2    = nn.Linear(d_ff, d_ff)\n","        self.fc_3    = nn.Linear(d_ff, 1)\n","\n","    def forward(self, inputs, input_emb):\n","        '''\n","        inputs: [batch_size, src_len, out_dim]\n","        '''\n","        output = torch.flatten(inputs, start_dim = 1)\n","        #output += input_emb.mean(dim = 1)\n","        output = output*self.weights + input_emb.mean(dim = 1) * (1-self.weights)\n","        #print(self.weights)\n","        output = self.fc_1(output)\n","        output = nn.functional.relu(output)\n","        last_layer = self.fc_2(output)\n","        output = nn.functional.relu(last_layer)\n","        output = self.fc_3(output)\n","\n","        return output, last_layer\n","\n","#====================================================================================================#\n","class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, d_k, n_heads, d_v, out_dim, d_ff): #out_dim = 1, n_head = 4, d_k = 256\n","        super(EncoderLayer, self).__init__()\n","        self.emb_self_attn = MultiHeadAttentionwithonekey(d_model, d_k, n_heads, d_v, out_dim)\n","        self.pos_ffn = PoswiseFeedForwardNet(d_model, d_ff)\n","\n","    def forward(self, input_emb, emb_self_attn_mask, input_mask):\n","        '''\n","        input_emb: [batch_size, src_len, d_model]\n","        emb_self_attn_mask: [batch_size, src_len, src_len]\n","        '''\n","        # output_emb: [batch_size, src_len, 1], attn: [batch_size, n_heads, src_len, src_len]\n","        output_emb, attn = self.emb_self_attn(input_emb, input_emb, input_emb, emb_self_attn_mask) # input_emb to same Q,K,V\n","        batch_mask = input_mask.unsqueeze(2)\n","        output_emb = output_emb * batch_mask\n","        pos_weights = nn.Softmax(dim = 1)(output_emb.masked_fill_(input_mask.unsqueeze(2).data.eq(0), -1e9)).permute(0,2,1) # [ batch_size, 1, src_len]\n","        output_emb = torch.matmul(pos_weights, input_emb)\n","        output_emb, last_layer = self.pos_ffn(output_emb, input_emb) # output_emb: [batch_size, d_model]\n","        return output_emb, last_layer\n","\n","#====================================================================================================#\n","# X05B\n","class SQembSAtt_Model(nn.Module):\n","    def __init__(self, d_model, d_k, n_heads, d_v, out_dim, d_ff):\n","        super(SQembSAtt_Model, self).__init__()\n","        self.layers = EncoderLayer(d_model, d_k, n_heads, d_v, out_dim, d_ff)\n","\n","    def get_attn_pad_mask(self, seq_mask):\n","        batch_size, len_q = seq_mask.size()\n","        _, len_k = seq_mask.size()\n","        # eq(zero) is PAD token\n","        pad_attn_mask = seq_mask.data.eq(0).unsqueeze(1)  # [batch_size, 1, len_k], True is masked\n","        return pad_attn_mask.expand(batch_size, len_q, len_k)\n","\n","    def forward(self, input_emb, input_mask):\n","        '''\n","        input_emb  : [batch_size, src_len, embedding_dim]\n","        input_mask : [batch_size, src_len]\n","        '''\n","        emb_self_attn_mask = self.get_attn_pad_mask(input_mask) # [batch_size, src_len, src_len]\n","        # output_emb: [batch_size, src_len, out_dim], emb_self_attn: [batch_size, n_heads, src_len, src_len]\n","        output_emb, last_layer = self.layers(input_emb, emb_self_attn_mask, input_mask)\n","        return output_emb, last_layer"],"metadata":{"id":"wwumKKf0Lbi-","executionInfo":{"status":"ok","timestamp":1706403785678,"user_tz":-330,"elapsed":22,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["d_k = 256\n","n_heads = 4\n","d_v = 256\n","out_dim = 1\n","d_ff = 1280\n","dropout = 0.\n","epoch_num      = 100\n","batch_size     = 256\n","learning_rate  =  [0.01        , # 0\n","                   0.005       , # 1\n","                   0.002       , # 2\n","                   0.001       , # 3\n","                   0.0005      , # 4\n","                   0.0002      , # 5\n","                   0.0001      , # 6\n","                   0.00005     , # 7\n","                   0.00002     , # 8\n","                   0.00001     , # 9\n","                   0.000005    , # 10\n","                   0.000002    , # 11\n","                   0.000001    , # 12\n","                   ][5]"],"metadata":{"id":"x8hDKtXEMKBs","executionInfo":{"status":"ok","timestamp":1706403785678,"user_tz":-330,"elapsed":19,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["model = SQembSAtt_Model(\n","    d_model=1280,\n","    d_k=d_k,\n","    n_heads=n_heads,\n","    d_v=d_v,\n","    out_dim=out_dim,\n","    d_ff=d_ff\n",")\n","\n","model.float()\n","model.cuda()\n","#--------------------------------------------------#\n","print(\"#\"*50)\n","print(model)\n","#model.float()\n","#--------------------------------------------------#"],"metadata":{"id":"H3C495YnMWOV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706403785679,"user_tz":-330,"elapsed":20,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"7d51bbe7-1952-4c1e-f9b7-c4fbe17d9ee9"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","SQembSAtt_Model(\n","  (layers): EncoderLayer(\n","    (emb_self_attn): MultiHeadAttentionwithonekey(\n","      (W_Q): Linear(in_features=1280, out_features=1024, bias=False)\n","      (W_K): Linear(in_features=1280, out_features=1024, bias=False)\n","      (W_V): Linear(in_features=1280, out_features=1024, bias=False)\n","      (fc_att): Linear(in_features=1024, out_features=1, bias=False)\n","    )\n","    (pos_ffn): PoswiseFeedForwardNet(\n","      (fc): Sequential(\n","        (0): Linear(in_features=1280, out_features=1280, bias=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=1280, out_features=1, bias=True)\n","      )\n","      (fc_1): Linear(in_features=1280, out_features=1280, bias=True)\n","      (fc_2): Linear(in_features=1280, out_features=1280, bias=True)\n","      (fc_3): Linear(in_features=1280, out_features=1, bias=True)\n","    )\n","  )\n",")\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Optional: Define a learning rate scheduler\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50], gamma=0.5)\n","\n","# Loss function\n","criterion = nn.MSELoss()"],"metadata":{"id":"k_eVq0jER4FC","executionInfo":{"status":"ok","timestamp":1706403786476,"user_tz":-330,"elapsed":815,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"id":"xK8gjcHSYcb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706403786476,"user_tz":-330,"elapsed":4,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"685c24c2-1fd3-4a78-f24f-002c9555bf53"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SQembSAtt_Model(\n","  (layers): EncoderLayer(\n","    (emb_self_attn): MultiHeadAttentionwithonekey(\n","      (W_Q): Linear(in_features=1280, out_features=1024, bias=False)\n","      (W_K): Linear(in_features=1280, out_features=1024, bias=False)\n","      (W_V): Linear(in_features=1280, out_features=1024, bias=False)\n","      (fc_att): Linear(in_features=1024, out_features=1, bias=False)\n","    )\n","    (pos_ffn): PoswiseFeedForwardNet(\n","      (fc): Sequential(\n","        (0): Linear(in_features=1280, out_features=1280, bias=True)\n","        (1): ReLU()\n","        (2): Linear(in_features=1280, out_features=1, bias=True)\n","      )\n","      (fc_1): Linear(in_features=1280, out_features=1280, bias=True)\n","      (fc_2): Linear(in_features=1280, out_features=1280, bias=True)\n","      (fc_3): Linear(in_features=1280, out_features=1, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["def save_plot(y_pred, y_real, epoch, folder_path, file_name, fig_size=(12, 10), marker_size=10, fit_line_color=\"skyblue\", distn_color_1=\"lightgreen\", distn_color_2=\"salmon\"):\n","    sns.set(style=\"whitegrid\")\n","\n","    # Create a jointplot\n","    g = sns.jointplot(\n","        x=y_real,\n","        y=y_pred,\n","        kind=\"reg\",\n","        height=fig_size[1] - 1,\n","        color=fit_line_color,\n","        scatter_kws={\"s\": marker_size},\n","        marginal_kws={'color': distn_color_1}\n","    )\n","\n","    # Set axis labels\n","    g.ax_joint.set_xlabel(\"Actual Values\", fontsize=12)\n","    g.ax_joint.set_ylabel(\"Predictions\", fontsize=12)\n","\n","    # Set the title with a bit more space at the top\n","    g.fig.suptitle(f\"Predictions vs. Actual Values\\n R = {np.round(stats.pearsonr(y_pred, y_real)[0], 3)} | Epoch: {epoch}\", fontsize=14, y=1.05)\n","\n","    # Plot the histograms\n","    sns.histplot(y_real, color=distn_color_1, alpha=0.6, ax=g.ax_marg_x, fill=True, kde=True)\n","    sns.histplot(y=y_pred, color=distn_color_2, alpha=0.6, ax=g.ax_marg_y, fill=True, kde=True)\n","\n","    # Adjust the plot margins and layout\n","    plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)\n","    g.fig.tight_layout()\n","\n","    # Save the plot\n","    plt.savefig(Path(folder_path) / file_name, bbox_inches='tight')\n","    plt.close(g.fig)"],"metadata":{"id":"hYmKODhGPD3A","executionInfo":{"status":"ok","timestamp":1706403786476,"user_tz":-330,"elapsed":3,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Training\n","model_name = 'att'\n","save_path = f'{root_path}/{model_name}_results/split_name_{split_name}/seed_{seed}'\n","test_interval = 5\n","best_r_score = -1\n","training_losses = []  # To store training losses for plotting\n","\n","# Variables to keep track of the best file names\n","best_model_file = \"\"\n","best_plot_file = \"\"\n","best_metrics_file = \"\"\n","for epoch in range(epoch_num):\n","    begin_time = time.time()\n","\n","    # Training Phase\n","    model.train()\n","    total_loss = 0\n","    for one_seq_ppt_group in train_loader:\n","        embeddings, mask, target = one_seq_ppt_group['embeddings'], one_seq_ppt_group['masks'], one_seq_ppt_group['targets']\n","        embeddings, mask, target = embeddings.float().to(device), mask.float().to(device), target.float().to(device)\n","\n","        optimizer.zero_grad()\n","        output, _ = model(embeddings, mask)\n","        loss = criterion(output, target.view(-1,1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    training_losses.append(avg_loss)\n","\n","    # Test Phase\n","    if epoch % test_interval == 0 or epoch == epoch_num - 1:\n","        model.eval()\n","        y_pred_test = []\n","        y_real_test = []\n","        for one_seq_ppt_group in test_loader:\n","            embeddings, mask, target = one_seq_ppt_group['embeddings'], one_seq_ppt_group['masks'], one_seq_ppt_group['targets']\n","            embeddings, mask, target = embeddings.float().to(device), mask.float().to(device), target.float().to(device)\n","\n","            output, _ = model(embeddings, mask)\n","            output = output.cpu().detach().numpy().reshape(-1)\n","            target = target.cpu().numpy()\n","\n","            y_pred_test.extend(output)\n","            y_real_test.extend(target)\n","\n","        _, _, r_value_test, _, _ = scipy.stats.linregress(y_pred_test, y_real_test)\n","\n","        # Save best model\n","        if r_value_test >= best_r_score:\n","            best_r_score = r_value_test\n","            os.makedirs(save_path, exist_ok=True)\n","            if best_model_file:\n","                os.remove(best_model_file)\n","                os.remove(best_plot_file)\n","                os.remove(best_metrics_file)\n","\n","            # Update file names\n","            best_model_file = f'{save_path}/best_model_epoch_{epoch}_seed_{seed}.pt'\n","            best_plot_file = f'{save_path}/test_plot_epoch_{epoch}.png'\n","            best_metrics_file = f'{save_path}/metrics_epoch_{epoch}.txt'\n","            torch.save(model.state_dict(), best_model_file)\n","            save_plot(y_pred_test, y_real_test, epoch, save_path, best_plot_file.split('/')[-1])\n","            with open(best_metrics_file, 'w') as f:\n","                f.write(f'Epoch: {epoch}\\n')\n","                f.write(f'Test R-value: {r_value_test}\\n')\n","                f.write(f'Test MAE: {mean_absolute_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test MSE: {mean_squared_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test RMSE: {np.sqrt(mean_squared_error(y_pred_test, y_real_test))}\\n')\n","                f.write(f'Test R2: {r2_score(y_real_test, y_pred_test)}\\n')\n","                f.write(f'Test Spearman rho: {scipy.stats.spearmanr(y_pred_test, y_real_test)[0]}\\n')\n","                # Additional metrics can be added here\n","            print(f'Best R-value: {best_r_score:.4f}')\n","\n","    print(f'Epoch: {epoch} | Training Loss: {avg_loss:.4f}')\n","    scheduler.step()"],"metadata":{"id":"GWRykPU1O3c0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b31af2f-0849-4908-e039-27d410bb7f56","executionInfo":{"status":"ok","timestamp":1706416033881,"user_tz":-330,"elapsed":12247408,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Best R-value: 0.3796\n","Epoch: 0 | Training Loss: 2.4545\n","Epoch: 1 | Training Loss: 1.4754\n","Epoch: 2 | Training Loss: 1.0019\n","Epoch: 3 | Training Loss: 0.8687\n","Epoch: 4 | Training Loss: 0.7971\n","Best R-value: 0.6693\n","Epoch: 5 | Training Loss: 0.7437\n","Epoch: 6 | Training Loss: 0.6939\n","Epoch: 7 | Training Loss: 0.6500\n","Epoch: 8 | Training Loss: 0.6067\n","Epoch: 9 | Training Loss: 0.5600\n","Best R-value: 0.7259\n","Epoch: 10 | Training Loss: 0.5157\n","Epoch: 11 | Training Loss: 0.4846\n","Epoch: 12 | Training Loss: 0.4446\n","Epoch: 13 | Training Loss: 0.4154\n","Epoch: 14 | Training Loss: 0.3966\n","Best R-value: 0.7534\n","Epoch: 15 | Training Loss: 0.3631\n","Epoch: 16 | Training Loss: 0.3434\n","Epoch: 17 | Training Loss: 0.3219\n","Epoch: 18 | Training Loss: 0.3096\n","Epoch: 19 | Training Loss: 0.3042\n","Best R-value: 0.7746\n","Epoch: 20 | Training Loss: 0.2774\n","Epoch: 21 | Training Loss: 0.2628\n","Epoch: 22 | Training Loss: 0.2590\n","Epoch: 23 | Training Loss: 0.2450\n","Epoch: 24 | Training Loss: 0.2307\n","Best R-value: 0.7899\n","Epoch: 25 | Training Loss: 0.2185\n","Epoch: 26 | Training Loss: 0.2101\n","Epoch: 27 | Training Loss: 0.2032\n","Epoch: 28 | Training Loss: 0.1997\n","Epoch: 29 | Training Loss: 0.1984\n","Best R-value: 0.7973\n","Epoch: 30 | Training Loss: 0.1936\n","Epoch: 31 | Training Loss: 0.1876\n","Epoch: 32 | Training Loss: 0.1832\n","Epoch: 33 | Training Loss: 0.1756\n","Epoch: 34 | Training Loss: 0.1762\n","Best R-value: 0.7998\n","Epoch: 35 | Training Loss: 0.1703\n","Epoch: 36 | Training Loss: 0.1675\n","Epoch: 37 | Training Loss: 0.1623\n","Epoch: 38 | Training Loss: 0.1580\n","Epoch: 39 | Training Loss: 0.1571\n","Best R-value: 0.8087\n","Epoch: 40 | Training Loss: 0.1540\n","Epoch: 41 | Training Loss: 0.1497\n","Epoch: 42 | Training Loss: 0.1490\n","Epoch: 43 | Training Loss: 0.1425\n","Epoch: 44 | Training Loss: 0.1411\n","Best R-value: 0.8113\n","Epoch: 45 | Training Loss: 0.1405\n","Epoch: 46 | Training Loss: 0.1350\n","Epoch: 47 | Training Loss: 0.1322\n","Epoch: 48 | Training Loss: 0.1298\n","Epoch: 49 | Training Loss: 0.1270\n","Best R-value: 0.8139\n","Epoch: 50 | Training Loss: 0.1220\n","Epoch: 51 | Training Loss: 0.1217\n","Epoch: 52 | Training Loss: 0.1209\n","Epoch: 53 | Training Loss: 0.1189\n","Epoch: 54 | Training Loss: 0.1169\n","Best R-value: 0.8175\n","Epoch: 55 | Training Loss: 0.1152\n","Epoch: 56 | Training Loss: 0.1152\n","Epoch: 57 | Training Loss: 0.1125\n","Epoch: 58 | Training Loss: 0.1119\n","Epoch: 59 | Training Loss: 0.1102\n","Epoch: 60 | Training Loss: 0.1096\n","Epoch: 61 | Training Loss: 0.1080\n","Epoch: 62 | Training Loss: 0.1074\n","Epoch: 63 | Training Loss: 0.1072\n","Epoch: 64 | Training Loss: 0.1054\n","Best R-value: 0.8193\n","Epoch: 65 | Training Loss: 0.1041\n","Epoch: 66 | Training Loss: 0.1029\n","Epoch: 67 | Training Loss: 0.1006\n","Epoch: 68 | Training Loss: 0.1007\n","Epoch: 69 | Training Loss: 0.1004\n","Best R-value: 0.8210\n","Epoch: 70 | Training Loss: 0.0995\n","Epoch: 71 | Training Loss: 0.0981\n","Epoch: 72 | Training Loss: 0.0961\n","Epoch: 73 | Training Loss: 0.0949\n","Epoch: 74 | Training Loss: 0.0939\n","Best R-value: 0.8218\n","Epoch: 75 | Training Loss: 0.0951\n","Epoch: 76 | Training Loss: 0.0926\n","Epoch: 77 | Training Loss: 0.0905\n","Epoch: 78 | Training Loss: 0.0903\n","Epoch: 79 | Training Loss: 0.0911\n","Best R-value: 0.8247\n","Epoch: 80 | Training Loss: 0.0889\n","Epoch: 81 | Training Loss: 0.0868\n","Epoch: 82 | Training Loss: 0.0865\n","Epoch: 83 | Training Loss: 0.0857\n","Epoch: 84 | Training Loss: 0.0839\n","Best R-value: 0.8250\n","Epoch: 85 | Training Loss: 0.0837\n","Epoch: 86 | Training Loss: 0.0830\n","Epoch: 87 | Training Loss: 0.0825\n","Epoch: 88 | Training Loss: 0.0814\n","Epoch: 89 | Training Loss: 0.0801\n","Best R-value: 0.8252\n","Epoch: 90 | Training Loss: 0.0794\n","Epoch: 91 | Training Loss: 0.0783\n","Epoch: 92 | Training Loss: 0.0780\n","Epoch: 93 | Training Loss: 0.0769\n","Epoch: 94 | Training Loss: 0.0768\n","Best R-value: 0.8283\n","Epoch: 95 | Training Loss: 0.0751\n","Epoch: 96 | Training Loss: 0.0757\n","Epoch: 97 | Training Loss: 0.0738\n","Epoch: 98 | Training Loss: 0.0736\n","Epoch: 99 | Training Loss: 0.0719\n"]}]},{"cell_type":"code","source":["# Save the training plot\n","plt.figure()\n","plt.plot(training_losses)\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.savefig(f'{save_path}/training_loss_plot_seed_{seed}.png')\n","plt.close()\n","\n","print(f'Training Completed. Best Model Saved with R-score: {best_r_score}')"],"metadata":{"id":"N3I7mIPBO6xs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706416033881,"user_tz":-330,"elapsed":21,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"e087c3b9-1102-418d-ec82-1b580e7e2474"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Completed. Best Model Saved with R-score: 0.8283012306340282\n"]}]}]}