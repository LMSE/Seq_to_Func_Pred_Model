{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8245,"status":"ok","timestamp":1706315204758,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"un4AJzKlD92F"},"outputs":[],"source":["import os\n","import sys\n","import os.path\n","from sys import platform\n","from pathlib import Path\n","import re\n","import sys\n","import time\n","import copy\n","import math\n","import scipy\n","import torch\n","import pickle\n","import random\n","import argparse\n","import subprocess\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","from torch.utils import data\n","from torch.nn.utils.weight_norm import weight_norm\n","from sklearn import datasets\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.metrics import auc\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n","import seaborn as sns\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from copy import deepcopy\n","from ipywidgets import IntProgress\n","from datetime import datetime\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30326,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"F7mFiqY7ETuc","outputId":"1cc94248-134d-475f-b123-256580bd82b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"x61hl34tEUJ6"},"outputs":[],"source":["# import sys\n","# sys.path.append('/content/gdrive/MyDrive/function_predictor/code/')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"fxQSvBX3WrG_"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"KvNDGOoxblt1"},"outputs":[],"source":["class StandardScaler:\n","    \"\"\"A :class:`StandardScaler` normalizes the features of a dataset.\n","\n","    When it is fit on a dataset, the :class:`StandardScaler` learns the mean and standard deviation across the 0th axis.\n","    When transforming a dataset, the :class:`StandardScaler` subtracts the means and divides by the standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, means: np.ndarray = None, stds: np.ndarray = None, replace_nan_token: Any = None):\n","        \"\"\"\n","        :param means: An optional 1D numpy array of precomputed means.\n","        :param stds: An optional 1D numpy array of precomputed standard deviations.\n","        :param replace_nan_token: A token to use to replace NaN entries in the features.\n","        \"\"\"\n","        self.means = means\n","        self.stds = stds\n","        self.replace_nan_token = replace_nan_token\n","\n","    def fit(self, X: List[List[Optional[float]]]) -> 'StandardScaler':\n","        \"\"\"\n","        Learns means and standard deviations across the 0th axis of the data :code:`X`.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The fitted :class:`StandardScaler` (self).\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        self.means = np.nanmean(X, axis=0)\n","        self.stds = np.nanstd(X, axis=0)\n","        self.means = np.where(np.isnan(self.means), np.zeros(self.means.shape), self.means)\n","        self.stds = np.where(np.isnan(self.stds), np.ones(self.stds.shape), self.stds)\n","        self.stds = np.where(self.stds == 0, np.ones(self.stds.shape), self.stds)\n","\n","        return self\n","\n","    def transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Transforms the data by subtracting the means and dividing by the standard deviations.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = (X - self.means) / self.stds\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none.astype(np.float32)\n","\n","    def inverse_transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Performs the inverse transformation by multiplying by the standard deviations and adding the means.\n","\n","        :param X: A list of lists of floats.\n","        :return: The inverse transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = X * self.stds + self.means\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none.astype('float32')\n","\n","\n","def normalize_targets(y_data) -> StandardScaler:\n","    # For Future Use.\n","    \"\"\"\n","    Normalizes the targets of the dataset using a :class:`~chemprop.data.StandardScaler`.\n","\n","    The :class:`~chemprop.data.StandardScaler` subtracts the mean and divides by the standard deviation\n","    for each task independently.\n","\n","    This should only be used for regression datasets.\n","\n","    :return: A :class:`~chemprop.data.StandardScaler` fitted to the targets.\n","    \"\"\"\n","    scaler = StandardScaler().fit(y_data)\n","    scaled_targets = scaler.transform(y_data).tolist()\n","\n","    return scaled_targets, scaler"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"R5iON4nZEZET"},"outputs":[],"source":["seeds = [0,1,2,42,1234]\n","seed=seeds[0]\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706315235081,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"LXsSkRmJIByS"},"outputs":[],"source":["root_path = '/content/gdrive/MyDrive/function_predictor/GB1-Dataset-FewToMore'"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":112928,"status":"ok","timestamp":1706315348005,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"ma9yTAMDqgQm"},"outputs":[],"source":["embedding_file = root_path+'/'+'emb_residue_level_embedding_GB1_embedding_ESM_2_650_embeddings_tensor.pt'\n","embeddings = torch.load(embedding_file)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1706315348006,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"pUtWyEmfBD_y"},"outputs":[],"source":["splits = ['low_vs_high', 'one_vs_rest', 'two_vs_rest', 'three_vs_rest']\n","split_name = splits[-1]\n","fasta_file = root_path+'/'+split_name+'.fasta'"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1983,"status":"ok","timestamp":1706315349973,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"jfibCggIhFBe"},"outputs":[],"source":["# [New] Parse FASTA file and create datasets\n","sequences = {}\n","with open(fasta_file, 'r') as file:\n","    for line in file:\n","        if line.startswith('>'):\n","            name, target, set_info, _ = line.strip().split(' ')\n","            name = name[1:]\n","            target = float(target.split('=')[1])\n","            set_type = set_info.split('=')[1]\n","            sequences[name] = {'target': target, 'set': set_type}"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1706315349973,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"o5DVV5IBLqR9"},"outputs":[],"source":["train_data = []\n","test_data = []\n","for name, info in sequences.items():\n","    embedding = embeddings[f'{name}'].numpy()\n","    if info['set'] == 'train':\n","        train_data.append((embedding, info['target']))\n","    elif info['set'] == 'test':\n","        test_data.append((embedding, info['target']))"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1706315349974,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"frk_3apZkzWY"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n","from torch.nn.utils.rnn import pad_sequence\n","\n","class LSTM_dataset(Dataset):\n","    def __init__(self, data, max_len, X_scaler=None):\n","        super().__init__()\n","        self.data = data\n","        self.max_len = max_len\n","        self.X_scaler = X_scaler\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        embedding, target = self.data[idx]\n","        seq_len = len(embedding)\n","\n","        # Pad on-the-fly\n","        padded_embedding = np.zeros((self.max_len, embedding.shape[1]), dtype=np.float32)\n","        padded_embedding[:seq_len, :] = embedding\n","\n","        return {'embedding': padded_embedding, 'seq_len': seq_len, 'target': np.array(target, dtype=np.float32)}\n","\n","    def collate_fn(self, batch):\n","        embeddings = [item['embedding'] for item in batch]\n","        seq_lens = [item['seq_len'] for item in batch]\n","        targets = [item['target'] for item in batch]\n","        # Convert to numpy arrays\n","        embeddings_array = np.stack(embeddings)\n","        seq_lens_array = np.array(seq_lens, dtype=np.int64)\n","        targets_array = np.stack(targets)\n","\n","        # Reshape for scaling: (batch_size * max_len, emb_dim)\n","        flat_embeddings = embeddings_array.reshape(-1, embeddings_array.shape[-1])\n","        if self.X_scaler is not None:\n","            scaled_flat_embeddings = self.X_scaler.transform(flat_embeddings)\n","        else:\n","            scaled_flat_embeddings = flat_embeddings\n","        scaled_embeddings = scaled_flat_embeddings.reshape(embeddings_array.shape)\n","\n","        # Convert to tensors\n","        embeddings_tensor = torch.from_numpy(scaled_embeddings)\n","        seq_lens_tensor = torch.from_numpy(seq_lens_array)\n","        targets_tensor = torch.from_numpy(targets_array)\n","\n","        return {\n","            'embeddings': embeddings_tensor,\n","            'seq_lens': seq_lens_tensor,\n","            'targets': targets_tensor\n","        }\n","def generate_LSTM_loader(train_data, test_data, max_len, batch_size, scaler: Optional[StandardScaler] = None):\n","    # Fit the scaler on all embeddings from the training data\n","    flat_train_embeddings = np.concatenate([item[0].reshape(-1, 1280) for item in train_data], axis=0)\n","\n","    if scaler is None:\n","        scaler = SklearnStandardScaler()\n","    scaler.fit(flat_train_embeddings)\n","\n","    train_dataset = LSTM_dataset(train_data, max_len, scaler)\n","    test_dataset = LSTM_dataset(test_data, max_len, scaler)\n","\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n","    test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=test_dataset.collate_fn)\n","\n","    return train_loader, test_loader"]},{"cell_type":"code","source":["y_train_normalized, target_scaler = normalize_targets([item[1] for item in train_data])\n","y_test_normalized = target_scaler.transform([item[1] for item in test_data])\n","\n","# Update train and test data with normalized targets\n","train_data_normalized = [(item[0], y_train_normalized[i]) for i, item in enumerate(train_data)]\n","test_data_normalized = [(item[0], y_test_normalized[i]) for i, item in enumerate(test_data)]"],"metadata":{"id":"cJMJDyKrQnsl","executionInfo":{"status":"ok","timestamp":1706315349974,"user_tz":-330,"elapsed":7,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":20053,"status":"ok","timestamp":1706315370020,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"EFeiFZzlk3mB"},"outputs":[],"source":["# Example usage\n","max_len = 265  # Set your sequence max length here\n","scaler = StandardScaler()  # Initialize your custom scaler\n","# train_loader, test_loader = generate_LSTM_loader(train_data_normalized, test_data_normalized, max_len, 256, scaler)\n","train_loader, test_loader = generate_LSTM_loader(train_data, test_data, max_len, 256, scaler)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1706315370020,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"VdCUSTlG-rpy"},"outputs":[],"source":["# for one_seq_ppt_group in train_loader:\n","#     seq_rep, seq_lens, target = one_seq_ppt_group[\"embeddings\"], one_seq_ppt_group[\"seq_lens\"], one_seq_ppt_group[\"targets\"]\n","#     break"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706315370020,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"wwumKKf0Lbi-"},"outputs":[],"source":["class LSTM(nn.Module):\n","    def __init__(self,\n","                 in_dim: int,\n","                 hid_dim: int,\n","                 latent_dim: int,\n","                 out_dim: int,\n","                 num_layers: int,\n","                 max_len: int,\n","                 last_hid: int,\n","                 dropout: float = 0.\n","                 ):\n","        super().__init__()\n","        self.latent_dim = latent_dim\n","        self.num_layers = num_layers\n","        self.hid_dim = hid_dim\n","        #--------------------------------------------------#\n","        self.encoder_LSTM = nn.LSTM(in_dim, hid_dim, batch_first=True, num_layers=num_layers)\n","        self.mean = nn.Linear(in_features=hid_dim*num_layers, out_features=latent_dim)\n","        self.log_variance = nn.Linear(in_features=hid_dim*num_layers, out_features = latent_dim)\n","        self.dropout = nn.Dropout(dropout, inplace=True)\n","        #--------------------------------------------------#\n","        self.fc_1 = nn.Linear(int(self.latent_dim),last_hid)\n","        self.fc_2 = nn.Linear(last_hid,last_hid)\n","        self.fc_3 = nn.Linear(last_hid,1)\n","        self.cls = nn.Sigmoid()\n","\n","    def encoder(self, x_inputs, padding_length, hidden_encoder):\n","        # Pad the packed input (already done when input into the NN)\n","        packed_output_encoder, hidden_encoder = self.encoder_LSTM(x_inputs, hidden_encoder)\n","        output_encoder, _ = nn.utils.rnn.pad_packed_sequence(packed_output_encoder, batch_first=True, total_length=padding_length)\n","        output_encoder.cuda()\n","\n","        # Estimate the mean and the variance\n","        mean = self.mean(hidden_encoder[0]).cuda()\n","        log_var = self.log_variance(hidden_encoder[0])\n","        std = torch.exp(0.5*log_var).cuda()\n","\n","        output_encoder = output_encoder.contiguous().cuda()\n","\n","        # Generating unit Gaussian noise\n","        batch_size = output_encoder.shape[0]\n","        seq_len = output_encoder.shape[1]\n","        noise = torch.randn(batch_size, self.latent_dim).cuda()\n","\n","        z = noise*std + mean\n","        #print(\"Z DIMENSION:\", z.shape)\n","        return z, mean, log_var, hidden_encoder\n","\n","    def initial_hidden_vars(self, batch_size):\n","        hidden_cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).float().cuda()\n","        state_cell = torch.zeros(self.num_layers, batch_size, self.hid_dim).float().cuda()\n","\n","        return (hidden_cell, state_cell)\n","\n","\n","    def forward(self, x, lengths, hidden_encoder):\n","        max_length = x.shape[1]\n","        #hidden_cell = torch.zeros(self.num_layers, x.shape[0], self.hid_dim)\n","        #state_cell = torch.zeros(self.num_layers, x.shape[0], self.hid_dim)\n","        lengths = lengths.cpu()\n","        x = nn.utils.rnn.pack_padded_sequence(input=x, lengths=lengths, batch_first=True, enforce_sorted=False)\n","        z, mean, log_var, hidden_encoder = self.encoder(x, max_length, hidden_encoder)\n","        #print(\"Z DIMENSION:\", z.shape)\n","        #--------------------------------------------------#\n","        #output = nn.functional.relu(z)\n","        #output = self.dropout1(output)\n","        #--------------------------------------------------#\n","        output = self.fc_1(z)\n","        output = nn.functional.relu(output)\n","        output = self.fc_2(output)\n","        output = nn.functional.relu(output)\n","        output = self.fc_3(output)\n","        return output"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1706315370020,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"x8hDKtXEMKBs"},"outputs":[],"source":["hid_dim    = 256    # 256\n","latent_dim   = 1024      # 5\n","out_dim    = 1      # 2\n","num_layers   = 1      # 3\n","last_hid   = 1280  # 1024\n","dropout    = 0.0     # 0\n","seqs_max_len = 265\n","epoch_num      = 100\n","batch_size     = 256\n","learning_rate  =  [0.01        , # 0\n","                   0.005       , # 1\n","                   0.002       , # 2\n","                   0.001       , # 3\n","                   0.0005      , # 4\n","                   0.0002      , # 5\n","                   0.0001      , # 6\n","                   0.00005     , # 7\n","                   0.00002     , # 8\n","                   0.00001     , # 9\n","                   0.000005    , # 10\n","                   0.000002    , # 11\n","                   0.000001    , # 12\n","                   ][6]"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1140,"status":"ok","timestamp":1706315371147,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"H3C495YnMWOV","outputId":"0351182b-3b70-4726-a9bc-ffc0b6aa1556"},"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","LSTM(\n","  (encoder_LSTM): LSTM(1280, 256, batch_first=True)\n","  (mean): Linear(in_features=256, out_features=1024, bias=True)\n","  (log_variance): Linear(in_features=256, out_features=1024, bias=True)\n","  (dropout): Dropout(p=0.0, inplace=True)\n","  (fc_1): Linear(in_features=1024, out_features=1280, bias=True)\n","  (fc_2): Linear(in_features=1280, out_features=1280, bias=True)\n","  (fc_3): Linear(in_features=1280, out_features=1, bias=True)\n","  (cls): Sigmoid()\n",")\n","##################################################\n"]}],"source":["model = LSTM(\n","            in_dim    =  1280         ,\n","            hid_dim   =  hid_dim      ,\n","            latent_dim  =  latent_dim ,\n","            out_dim   =  out_dim      ,\n","            num_layers  =  num_layers ,\n","            max_len   =  seqs_max_len ,\n","            last_hid  =  last_hid     ,\n","            dropout   =  dropout      ,\n","            )\n","\n","model.float()\n","model.cuda()\n","#--------------------------------------------------#\n","print(\"#\"*50)\n","print(model)\n","#model.float()\n","#print( summary( model,[(seqs_max_len, NN_input_dim),] )  )\n","#model.float()\n","print(\"#\"*50)\n","#--------------------------------------------------#\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Optional: Define a learning rate scheduler\n","# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50], gamma=0.5)\n","\n","# Loss function\n","criterion = nn.MSELoss()"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1706315371147,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"xK8gjcHSYcb_","outputId":"356d359d-1d4f-4f47-fe9d-7575600d7ab4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["LSTM(\n","  (encoder_LSTM): LSTM(1280, 256, batch_first=True)\n","  (mean): Linear(in_features=256, out_features=1024, bias=True)\n","  (log_variance): Linear(in_features=256, out_features=1024, bias=True)\n","  (dropout): Dropout(p=0.0, inplace=True)\n","  (fc_1): Linear(in_features=1024, out_features=1280, bias=True)\n","  (fc_2): Linear(in_features=1280, out_features=1280, bias=True)\n","  (fc_3): Linear(in_features=1280, out_features=1, bias=True)\n","  (cls): Sigmoid()\n",")"]},"metadata":{},"execution_count":19}],"source":["model.to(device)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1706315371147,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"},"user_tz":-330},"id":"hYmKODhGPD3A"},"outputs":[],"source":["def save_plot(y_pred, y_real, epoch, folder_path, file_name, fig_size=(12, 10), marker_size=10, fit_line_color=\"skyblue\", distn_color_1=\"lightgreen\", distn_color_2=\"salmon\"):\n","    sns.set(style=\"whitegrid\")\n","\n","    # Create a jointplot\n","    g = sns.jointplot(\n","        x=y_real,\n","        y=y_pred,\n","        kind=\"reg\",\n","        height=fig_size[1] - 1,\n","        color=fit_line_color,\n","        scatter_kws={\"s\": marker_size},\n","        marginal_kws={'color': distn_color_1}\n","    )\n","\n","    # Set axis labels\n","    g.ax_joint.set_xlabel(\"Actual Values\", fontsize=12)\n","    g.ax_joint.set_ylabel(\"Predictions\", fontsize=12)\n","\n","    # Set the title with a bit more space at the top\n","    g.fig.suptitle(f\"Predictions vs. Actual Values\\n R = {np.round(stats.pearsonr(y_pred, y_real)[0], 3)} | Epoch: {epoch}\", fontsize=14, y=1.05)\n","\n","    # Plot the histograms\n","    sns.histplot(y_real, color=distn_color_1, alpha=0.6, ax=g.ax_marg_x, fill=True, kde=True)\n","    sns.histplot(y=y_pred, color=distn_color_2, alpha=0.6, ax=g.ax_marg_y, fill=True, kde=True)\n","\n","    # Adjust the plot margins and layout\n","    plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)\n","    g.fig.tight_layout()\n","\n","    # Save the plot\n","    plt.savefig(Path(folder_path) / file_name, bbox_inches='tight')\n","    plt.close(g.fig)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWRykPU1O3c0","executionInfo":{"status":"ok","timestamp":1706327448642,"user_tz":-330,"elapsed":12077498,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"9dfc50f9-5ed0-4765-f3ed-75c7d6922dba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Best R-value: 0.0000\n","Epoch: 0 | Training Loss: 1.5116\n","Epoch: 1 | Training Loss: 1.2417\n","Epoch: 2 | Training Loss: 1.2156\n","Epoch: 3 | Training Loss: 1.2018\n","Epoch: 4 | Training Loss: 1.2140\n","Best R-value: 0.0178\n","Epoch: 5 | Training Loss: 1.1876\n","Epoch: 6 | Training Loss: 1.2042\n","Epoch: 7 | Training Loss: 1.2072\n","Epoch: 8 | Training Loss: 1.2039\n","Epoch: 9 | Training Loss: 1.2056\n","Best R-value: 0.0651\n","Epoch: 10 | Training Loss: 1.2088\n","Epoch: 11 | Training Loss: 1.2054\n","Epoch: 12 | Training Loss: 1.1899\n","Epoch: 13 | Training Loss: 1.1866\n","Epoch: 14 | Training Loss: 1.1769\n","Best R-value: 0.1898\n","Epoch: 15 | Training Loss: 1.1839\n","Epoch: 16 | Training Loss: 1.1690\n","Epoch: 17 | Training Loss: 1.1454\n","Epoch: 18 | Training Loss: 1.1812\n","Epoch: 19 | Training Loss: 1.1406\n","Best R-value: 0.3191\n","Epoch: 20 | Training Loss: 1.1261\n","Epoch: 21 | Training Loss: 1.1463\n","Epoch: 22 | Training Loss: 1.1336\n","Epoch: 23 | Training Loss: 1.1137\n","Epoch: 24 | Training Loss: 1.1031\n","Epoch: 25 | Training Loss: 1.1348\n","Epoch: 26 | Training Loss: 1.1488\n","Epoch: 27 | Training Loss: 1.1585\n","Epoch: 28 | Training Loss: 1.1028\n","Epoch: 29 | Training Loss: 1.1086\n","Best R-value: 0.4076\n","Epoch: 30 | Training Loss: 1.0971\n","Epoch: 31 | Training Loss: 1.0856\n","Epoch: 32 | Training Loss: 1.1242\n","Epoch: 33 | Training Loss: 1.1092\n","Epoch: 34 | Training Loss: 1.0981\n","Best R-value: 0.4246\n","Epoch: 35 | Training Loss: 1.0932\n","Epoch: 36 | Training Loss: 1.1231\n","Epoch: 37 | Training Loss: 1.1547\n","Epoch: 38 | Training Loss: 1.1179\n","Epoch: 39 | Training Loss: 1.0831\n","Best R-value: 0.4434\n","Epoch: 40 | Training Loss: 1.0883\n","Epoch: 41 | Training Loss: 1.0843\n","Epoch: 42 | Training Loss: 1.0580\n","Epoch: 43 | Training Loss: 1.0595\n","Epoch: 44 | Training Loss: 1.0644\n","Best R-value: 0.4763\n","Epoch: 45 | Training Loss: 1.0442\n","Epoch: 46 | Training Loss: 1.0330\n","Epoch: 47 | Training Loss: 1.0491\n","Epoch: 48 | Training Loss: 1.1219\n","Epoch: 49 | Training Loss: 1.0322\n","Epoch: 50 | Training Loss: 1.0607\n","Epoch: 51 | Training Loss: 1.0613\n","Epoch: 52 | Training Loss: 1.0828\n","Epoch: 53 | Training Loss: 1.0614\n","Epoch: 54 | Training Loss: 1.0904\n","Epoch: 55 | Training Loss: 1.0378\n","Epoch: 56 | Training Loss: 1.0569\n","Epoch: 57 | Training Loss: 1.0514\n","Epoch: 58 | Training Loss: 1.0065\n","Epoch: 59 | Training Loss: 1.0038\n","Best R-value: 0.4902\n","Epoch: 60 | Training Loss: 1.0267\n","Epoch: 61 | Training Loss: 1.0436\n","Epoch: 62 | Training Loss: 1.1093\n","Epoch: 63 | Training Loss: 1.0716\n","Epoch: 64 | Training Loss: 1.0098\n","Best R-value: 0.4915\n","Epoch: 65 | Training Loss: 1.0102\n","Epoch: 66 | Training Loss: 1.0286\n","Epoch: 67 | Training Loss: 1.0156\n","Epoch: 68 | Training Loss: 0.9893\n","Epoch: 69 | Training Loss: 1.0003\n","Epoch: 70 | Training Loss: 0.9732\n","Epoch: 71 | Training Loss: 0.9972\n","Epoch: 72 | Training Loss: 0.9843\n","Epoch: 73 | Training Loss: 0.9694\n","Epoch: 74 | Training Loss: 0.9631\n","Best R-value: 0.5144\n","Epoch: 75 | Training Loss: 1.0199\n","Epoch: 76 | Training Loss: 0.9753\n","Epoch: 77 | Training Loss: 0.9951\n","Epoch: 78 | Training Loss: 0.9608\n","Epoch: 79 | Training Loss: 1.0135\n","Epoch: 80 | Training Loss: 0.9634\n","Epoch: 81 | Training Loss: 0.9688\n","Epoch: 82 | Training Loss: 0.9692\n","Epoch: 83 | Training Loss: 1.0077\n","Epoch: 84 | Training Loss: 1.0047\n","Epoch: 85 | Training Loss: 1.0455\n","Epoch: 86 | Training Loss: 1.0272\n","Epoch: 87 | Training Loss: 1.0150\n","Epoch: 88 | Training Loss: 0.9927\n","Epoch: 89 | Training Loss: 0.9587\n","Best R-value: 0.5264\n","Epoch: 90 | Training Loss: 0.9462\n","Epoch: 91 | Training Loss: 0.9566\n","Epoch: 92 | Training Loss: 0.9924\n","Epoch: 93 | Training Loss: 0.9634\n","Epoch: 94 | Training Loss: 0.9421\n","Best R-value: 0.5488\n","Epoch: 95 | Training Loss: 0.9343\n","Epoch: 96 | Training Loss: 0.9496\n","Epoch: 97 | Training Loss: 0.9459\n","Epoch: 98 | Training Loss: 0.9552\n","Epoch: 99 | Training Loss: 0.9633\n"]}],"source":["# Training\n","model_name = 'lstm'\n","save_path = f'{root_path}/{model_name}_results/split_name_{split_name}/seed_{seed}'\n","test_interval = 5\n","best_r_score = -1\n","training_losses = []  # To store training losses for plotting\n","\n","# Variables to keep track of the best file names\n","best_model_file = \"\"\n","best_plot_file = \"\"\n","best_metrics_file = \"\"\n","for epoch in range(epoch_num):\n","    begin_time = time.time()\n","\n","    # Training Phase\n","    model.train()\n","    total_loss = 0\n","    for one_seq_ppt_group in train_loader:\n","        len_train_loader=batch_size\n","        seq_rep, seq_lens, target = one_seq_ppt_group[\"embeddings\"], one_seq_ppt_group[\"seq_lens\"], one_seq_ppt_group[\"targets\"]\n","        seq_rep, seq_lens, target = seq_rep.float().to(device), seq_lens.to(device), target.float().to(device)\n","        states = model.initial_hidden_vars(len_train_loader)\n","        input_vars = [seq_rep, seq_lens, states]\n","        output = model(*input_vars)\n","        loss = criterion(torch.squeeze(output),torch.squeeze(target))\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    training_losses.append(avg_loss)\n","\n","    # Test Phase\n","    if epoch % test_interval == 0 or epoch == epoch_num - 1:\n","        model.eval()\n","        y_pred_test = []\n","        y_real_test = []\n","        for one_seq_ppt_group in test_loader:\n","            len_val_loader = batch_size\n","            seq_rep, seq_lens, target = one_seq_ppt_group[\"embeddings\"], one_seq_ppt_group[\"seq_lens\"], one_seq_ppt_group[\"targets\"]\n","            seq_rep, seq_lens, target = seq_rep.float().to(device), seq_lens.to(device), target.float().to(device)\n","            states = model.initial_hidden_vars(len_val_loader)\n","            input_vars = [seq_rep, seq_lens, states]\n","            output = model(*input_vars)\n","            output = output.cpu().detach().numpy().reshape(-1)\n","            target = target.cpu().numpy()\n","\n","            y_pred_test.extend(output)\n","            y_real_test.extend(target)\n","\n","        _, _, r_value_test, _, _ = scipy.stats.linregress(y_pred_test, y_real_test)\n","\n","        # Save best model\n","        if r_value_test >= best_r_score:\n","            best_r_score = r_value_test\n","            os.makedirs(save_path, exist_ok=True)\n","            if best_model_file:\n","                os.remove(best_model_file)\n","                os.remove(best_plot_file)\n","                os.remove(best_metrics_file)\n","\n","            # Update file names\n","            best_model_file = f'{save_path}/best_model_epoch_{epoch}_seed_{seed}.pt'\n","            best_plot_file = f'{save_path}/test_plot_epoch_{epoch}.png'\n","            best_metrics_file = f'{save_path}/metrics_epoch_{epoch}.txt'\n","            torch.save(model.state_dict(), best_model_file)\n","            save_plot(y_pred_test, y_real_test, epoch, save_path, best_plot_file.split('/')[-1])\n","            with open(best_metrics_file, 'w') as f:\n","                f.write(f'Epoch: {epoch}\\n')\n","                f.write(f'Test R-value: {r_value_test}\\n')\n","                f.write(f'Test MAE: {mean_absolute_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test MSE: {mean_squared_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test RMSE: {np.sqrt(mean_squared_error(y_pred_test, y_real_test))}\\n')\n","                f.write(f'Test R2: {r2_score(y_real_test, y_pred_test)}\\n')\n","                f.write(f'Test Spearman rho: {scipy.stats.spearmanr(y_pred_test, y_real_test)[0]}\\n')\n","                # Additional metrics can be added here\n","            print(f'Best R-value: {best_r_score:.4f}')\n","\n","    print(f'Epoch: {epoch} | Training Loss: {avg_loss:.4f}')\n","    # scheduler.step()"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"N3I7mIPBO6xs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706327448643,"user_tz":-330,"elapsed":17,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"b79e5281-1588-44a5-8daa-2bc1c64d181a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Completed. Best Model Saved with R-score: 0.5488440681193696\n"]}],"source":["# Save the training plot\n","plt.figure()\n","plt.plot(training_losses)\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.savefig(f'{save_path}/training_loss_plot_seed_{seed}.png')\n","plt.close()\n","\n","print(f'Training Completed. Best Model Saved with R-score: {best_r_score}')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ZWpf3BerPb0a","executionInfo":{"status":"ok","timestamp":1706327448643,"user_tz":-330,"elapsed":15,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyM6HnFzobHi3XrBE+ZV8DMI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}