{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOAmr69wHwEb1t4vz+hFzfP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"un4AJzKlD92F"},"outputs":[],"source":["import os\n","import sys\n","import os.path\n","from sys import platform\n","from pathlib import Path\n","import re\n","import sys\n","import time\n","import copy\n","import math\n","import scipy\n","import torch\n","import pickle\n","import random\n","import argparse\n","import subprocess\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","from torch import nn\n","from torch.utils import data\n","from torch.nn.utils.weight_norm import weight_norm\n","from sklearn import datasets\n","from sklearn import preprocessing\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn import preprocessing\n","from sklearn.metrics import auc\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.preprocessing import StandardScaler\n","from typing import Union, List, Tuple, Sequence, Dict, Any, Optional, Collection\n","import seaborn as sns\n","from scipy import stats\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from copy import deepcopy\n","from ipywidgets import IntProgress\n","from datetime import datetime\n","from mpl_toolkits.axes_grid1 import make_axes_locatable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"metadata":{"id":"F7mFiqY7ETuc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706394095953,"user_tz":-330,"elapsed":24096,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"a4aa4f83-c8fa-4af7-e0c9-e7905f7c6852"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/function_predictor/code/')"],"metadata":{"id":"x61hl34tEUJ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"fxQSvBX3WrG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class StandardScaler:\n","    \"\"\"A :class:`StandardScaler` normalizes the features of a dataset.\n","\n","    When it is fit on a dataset, the :class:`StandardScaler` learns the mean and standard deviation across the 0th axis.\n","    When transforming a dataset, the :class:`StandardScaler` subtracts the means and divides by the standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, means: np.ndarray = None, stds: np.ndarray = None, replace_nan_token: Any = None):\n","        \"\"\"\n","        :param means: An optional 1D numpy array of precomputed means.\n","        :param stds: An optional 1D numpy array of precomputed standard deviations.\n","        :param replace_nan_token: A token to use to replace NaN entries in the features.\n","        \"\"\"\n","        self.means = means\n","        self.stds = stds\n","        self.replace_nan_token = replace_nan_token\n","\n","    def fit(self, X: List[List[Optional[float]]]) -> 'StandardScaler':\n","        \"\"\"\n","        Learns means and standard deviations across the 0th axis of the data :code:`X`.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The fitted :class:`StandardScaler` (self).\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        self.means = np.nanmean(X, axis=0)\n","        self.stds = np.nanstd(X, axis=0)\n","        self.means = np.where(np.isnan(self.means), np.zeros(self.means.shape), self.means)\n","        self.stds = np.where(np.isnan(self.stds), np.ones(self.stds.shape), self.stds)\n","        self.stds = np.where(self.stds == 0, np.ones(self.stds.shape), self.stds)\n","\n","        return self\n","\n","    def transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Transforms the data by subtracting the means and dividing by the standard deviations.\n","\n","        :param X: A list of lists of floats (or None).\n","        :return: The transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = (X - self.means) / self.stds\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none\n","\n","    def inverse_transform(self, X: List[List[Optional[float]]]) -> np.ndarray:\n","        \"\"\"\n","        Performs the inverse transformation by multiplying by the standard deviations and adding the means.\n","\n","        :param X: A list of lists of floats.\n","        :return: The inverse transformed data with NaNs replaced by :code:`self.replace_nan_token`.\n","        \"\"\"\n","        X = np.array(X).astype(float)\n","        transformed_with_nan = X * self.stds + self.means\n","        transformed_with_none = np.where(np.isnan(transformed_with_nan), self.replace_nan_token, transformed_with_nan)\n","\n","        return transformed_with_none.astype('float32')\n","\n","\n","def normalize_targets(y_data) -> StandardScaler:\n","    # For Future Use.\n","    \"\"\"\n","    Normalizes the targets of the dataset using a :class:`~chemprop.data.StandardScaler`.\n","\n","    The :class:`~chemprop.data.StandardScaler` subtracts the mean and divides by the standard deviation\n","    for each task independently.\n","\n","    This should only be used for regression datasets.\n","\n","    :return: A :class:`~chemprop.data.StandardScaler` fitted to the targets.\n","    \"\"\"\n","    scaler = StandardScaler().fit(y_data)\n","    scaled_targets = scaler.transform(y_data).tolist()\n","\n","    return scaled_targets, scaler"],"metadata":{"id":"KvNDGOoxblt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seeds = [0,1,2,42,1234]\n","seed=seeds[1]\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)"],"metadata":{"id":"R5iON4nZEZET"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root_path = '/content/gdrive/MyDrive/function_predictor/GB1-Dataset-FewToMore'"],"metadata":{"id":"LXsSkRmJIByS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_file = root_path+'/'+'emb_residue_level_embedding_GB1_embedding_ESM_2_650_embeddings_tensor.pt'\n","embeddings = torch.load(embedding_file)"],"metadata":{"id":"ma9yTAMDqgQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["splits = ['low_vs_high', 'one_vs_rest', 'two_vs_rest', 'three_vs_rest']\n","split_name = splits[-1]\n","fasta_file = root_path+'/'+split_name+'.fasta'"],"metadata":{"id":"pUtWyEmfBD_y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# [New] Parse FASTA file and create datasets\n","sequences = {}\n","with open(fasta_file, 'r') as file:\n","    for line in file:\n","        if line.startswith('>'):\n","            name, target, set_info, _ = line.strip().split(' ')\n","            name = name[1:]\n","            target = float(target.split('=')[1])\n","            set_type = set_info.split('=')[1]\n","            sequences[name] = {'target': target, 'set': set_type}"],"metadata":{"id":"jfibCggIhFBe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = []\n","test_data = []\n","for name, info in sequences.items():\n","    embedding = embeddings[f'{name}'].numpy()\n","    if info['set'] == 'train':\n","        train_data.append((embedding, info['target']))\n","    elif info['set'] == 'test':\n","        test_data.append((embedding, info['target']))"],"metadata":{"id":"o5DVV5IBLqR9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_data)"],"metadata":{"id":"UUsMpqa0jhLH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706394217998,"user_tz":-330,"elapsed":7,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"616a6a04-96af-4981-a2fd-71c872dcd750"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2990"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["len(test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ygchos0WA6Hp","executionInfo":{"status":"ok","timestamp":1706394217999,"user_tz":-330,"elapsed":5,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"856c1f0c-bad1-451d-bd82-f3f5c68e47ac"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5743"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler as SklearnStandardScaler\n","class CNN_dataset(Dataset):\n","    def __init__(self, data, max_len, X_scaler=None):\n","        super().__init__()\n","        self.data = data\n","        self.max_len = max_len\n","        self.X_scaler = X_scaler\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        embedding, target = self.data[idx]\n","\n","        # Reshape, scale and pad on-the-fly\n","        embedding = embedding.reshape(-1, 1280)\n","        if self.X_scaler is not None:\n","            embedding = self.X_scaler.transform(embedding)\n","        embedding = embedding.reshape(self.max_len, -1)\n","        padded_embedding = np.zeros((self.max_len, embedding.shape[1]))\n","        padded_embedding[:embedding.shape[0], :embedding.shape[1]] = embedding\n","\n","        return {'seqs_embeddings': torch.from_numpy(padded_embedding), 'y_property': torch.tensor(target)}\n","\n","def generate_CNN_loader(train_data, test_data, max_len, batch_size, scaler: Optional[StandardScaler] = None):\n","    # Fit the scaler if it's not already fitted\n","    X_tr = np.array([embedding.reshape(-1, 1280) for embedding, _ in train_data])\n","    X_tr = X_tr.reshape(-1, 1280)\n","    if scaler is None:\n","        scaler = SklearnStandardScaler()\n","    scaler.fit(X_tr)\n","\n","    # Use the same dataset class for both train and test datasets\n","    train_dataset = CNN_dataset(train_data, max_len, scaler)\n","    test_dataset = CNN_dataset(test_data, max_len, scaler)\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n","\n","    return train_loader, test_loader"],"metadata":{"id":"frk_3apZkzWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 265  # Set your sequence max length here\n","scaler = StandardScaler()  # Replace with any scaler you want\n","train_loader, test_loader = generate_CNN_loader(train_data, test_data, max_len, batch_size=256, scaler=scaler)"],"metadata":{"id":"EFeiFZzlk3mB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self,\n","                 in_dim: int,\n","                 hid_dim: int,\n","                 kernal_1: int,\n","                 out_dim: int,\n","                 kernal_2: int,\n","                 max_len: int,\n","                 last_hid: int,\n","                 dropout: float = 0.\n","                 ):\n","        super().__init__()\n","        self.norm = nn.BatchNorm1d(in_dim)\n","        self.conv1 = nn.Conv1d(in_dim, hid_dim, kernal_1, padding=int((kernal_1-1)/2))\n","        self.dropout1 = nn.Dropout(dropout, inplace=True)\n","        #--------------------------------------------------#\n","        self.conv2_1 = nn.Conv1d(hid_dim, out_dim, kernal_2, padding=int((kernal_2-1)/2))\n","        self.dropout2_1 = nn.Dropout(dropout, inplace=True)\n","        #--------------------------------------------------#\n","        self.conv2_2 = nn.Conv1d(hid_dim, hid_dim, kernal_2, padding=int((kernal_2-1)/2))\n","        self.dropout2_2 = nn.Dropout(dropout, inplace=True)\n","        #--------------------------------------------------#\n","        self.conv3 = nn.Conv1d(hid_dim, out_dim, kernal_2, padding=int((kernal_2-1)/2))\n","        self.dropout3 = nn.Dropout(dropout, inplace=True)\n","        #self.pooling = nn.MaxPool1d(3, stride=3,padding=1)\n","        #--------------------------------------------------#\n","        self.fc_1 = nn.Linear(int(2*max_len*out_dim),last_hid)\n","        self.fc_2 = nn.Linear(last_hid,last_hid)\n","        self.fc_3 = nn.Linear(last_hid,1)\n","        self.cls = nn.Sigmoid()\n","\n","    def forward(self, enc_inputs):\n","        #--------------------------------------------------#\n","        output = enc_inputs.transpose(1, 2)\n","        output = self.norm(output)\n","        output = nn.functional.relu(self.conv1(output))\n","        output = self.dropout1(output)\n","        #--------------------------------------------------#\n","        output_1 = nn.functional.relu(self.conv2_1(output))\n","        output_1 = self.dropout2_1(output_1)\n","        #--------------------------------------------------#\n","        output_2 = nn.functional.relu(self.conv2_2(output)) + output\n","        output_2 = self.dropout2_2(output_2)\n","        #--------------------------------------------------#\n","        output_2 = nn.functional.relu(self.conv3(output_2))\n","        output_2 = self.dropout3(output_2)\n","        #--------------------------------------------------#\n","        output = torch.cat((output_1,output_2),1)\n","        #print(output.size())\n","        #--------------------------------------------------#\n","        #output = self.pooling(output)\n","        #--------------------------------------------------#\n","        output = torch.flatten(output,1)\n","        #print(output.size())\n","        #--------------------------------------------------#\n","        output = self.fc_1(output)\n","        output = nn.functional.relu(output)\n","        output = self.fc_2(output)\n","        output = nn.functional.relu(output)\n","        output = self.fc_3(output)\n","        return output, output_1"],"metadata":{"id":"wwumKKf0Lbi-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["NN_input_dim = 1280 #embedding length\n","hid_dim    = 512    # 256\n","kernal_1   = 3      # 5\n","out_dim    = 1      # 2\n","kernal_2   = 3      # 3\n","last_hid   = 1024   # 1024\n","dropout    = 0.0\n","epoch_num      = 100\n","batch_size     = 256\n","learning_rate  =  [0.01        , # 0\n","                   0.005       , # 1\n","                   0.002       , # 2\n","                   0.001       , # 3\n","                   0.0005      , # 4\n","                   0.0002      , # 5\n","                   0.0001      , # 6\n","                   0.00005     , # 7\n","                   0.00002     , # 8\n","                   0.00001     , # 8\n","                   0.000005    , # 10\n","                   0.000002    , # 11\n","                   0.000001    , # 12\n","                   ][7]"],"metadata":{"id":"x8hDKtXEMKBs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CNN(\n","            in_dim    =  NN_input_dim ,\n","            hid_dim   =  hid_dim      ,\n","            kernal_1  =  kernal_1     ,\n","            out_dim   =  out_dim      ,\n","            kernal_2  =  kernal_2     ,\n","            max_len   =  max_len ,\n","            last_hid  =  last_hid     ,\n","            dropout   =  dropout      ,\n","            )\n","\n","model.float()\n","model.cuda()\n","#--------------------------------------------------#\n","print(\"#\"*50)\n","print(model)\n","#model.float()\n","#print( summary( model,[(seqs_max_len, NN_input_dim),] )  )\n","#model.float()\n","print(\"#\"*50)\n","#--------------------------------------------------#\n","optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n","#optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n","criterion = nn.MSELoss()"],"metadata":{"id":"H3C495YnMWOV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706394239271,"user_tz":-330,"elapsed":858,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"cb5036a1-ad76-4c6a-80ec-346c92e92882"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["##################################################\n","CNN(\n","  (norm): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv1): Conv1d(1280, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout1): Dropout(p=0.0, inplace=True)\n","  (conv2_1): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout2_1): Dropout(p=0.0, inplace=True)\n","  (conv2_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout2_2): Dropout(p=0.0, inplace=True)\n","  (conv3): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout3): Dropout(p=0.0, inplace=True)\n","  (fc_1): Linear(in_features=530, out_features=1024, bias=True)\n","  (fc_2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (fc_3): Linear(in_features=1024, out_features=1, bias=True)\n","  (cls): Sigmoid()\n",")\n","##################################################\n"]}]},{"cell_type":"code","source":["model.to(device)"],"metadata":{"id":"xK8gjcHSYcb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706394239272,"user_tz":-330,"elapsed":12,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"6372baff-b6d3-416f-d628-775659d290a3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CNN(\n","  (norm): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (conv1): Conv1d(1280, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout1): Dropout(p=0.0, inplace=True)\n","  (conv2_1): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout2_1): Dropout(p=0.0, inplace=True)\n","  (conv2_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout2_2): Dropout(p=0.0, inplace=True)\n","  (conv3): Conv1d(512, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n","  (dropout3): Dropout(p=0.0, inplace=True)\n","  (fc_1): Linear(in_features=530, out_features=1024, bias=True)\n","  (fc_2): Linear(in_features=1024, out_features=1024, bias=True)\n","  (fc_3): Linear(in_features=1024, out_features=1, bias=True)\n","  (cls): Sigmoid()\n",")"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["def save_plot(y_pred, y_real, epoch, folder_path, file_name, fig_size=(12, 10), marker_size=10, fit_line_color=\"skyblue\", distn_color_1=\"lightgreen\", distn_color_2=\"salmon\"):\n","    sns.set(style=\"whitegrid\")\n","\n","    # Create a jointplot\n","    g = sns.jointplot(\n","        x=y_real,\n","        y=y_pred,\n","        kind=\"reg\",\n","        height=fig_size[1] - 1,\n","        color=fit_line_color,\n","        scatter_kws={\"s\": marker_size},\n","        marginal_kws={'color': distn_color_1}\n","    )\n","\n","    # Set axis labels\n","    g.ax_joint.set_xlabel(\"Actual Values\", fontsize=12)\n","    g.ax_joint.set_ylabel(\"Predictions\", fontsize=12)\n","\n","    # Set the title with a bit more space at the top\n","    g.fig.suptitle(f\"Predictions vs. Actual Values\\n R = {np.round(stats.pearsonr(y_pred, y_real)[0], 3)} | Epoch: {epoch}\", fontsize=14, y=1.05)\n","\n","    # Plot the histograms\n","    sns.histplot(y_real, color=distn_color_1, alpha=0.6, ax=g.ax_marg_x, fill=True, kde=True)\n","    sns.histplot(y=y_pred, color=distn_color_2, alpha=0.6, ax=g.ax_marg_y, fill=True, kde=True)\n","\n","    # Adjust the plot margins and layout\n","    plt.subplots_adjust(left=0.15, right=0.85, top=0.85, bottom=0.15)\n","    g.fig.tight_layout()\n","\n","    # Save the plot\n","    plt.savefig(Path(folder_path) / file_name, bbox_inches='tight')\n","    plt.close(g.fig)"],"metadata":{"id":"hYmKODhGPD3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training\n","model_name = 'cnn'\n","save_path = f'{root_path}/{model_name}_results/split_name_{split_name}/seed_{seed}'\n","test_interval = 5\n","best_r_score = -1\n","training_losses = []  # To store training losses for plotting\n","\n","# Variables to keep track of the best file names\n","best_model_file = \"\"\n","best_plot_file = \"\"\n","best_metrics_file = \"\"\n","for epoch in range(epoch_num):\n","    begin_time = time.time()\n","\n","    # Training Phase\n","    model.train()\n","    total_loss = 0\n","    for one_seq_ppt_group in train_loader:\n","        seq_rep, target = one_seq_ppt_group[\"seqs_embeddings\"], one_seq_ppt_group[\"y_property\"]\n","        seq_rep, target = seq_rep.float().to(device), target.float().to(device)\n","\n","\n","        optimizer.zero_grad()\n","        output, _ = model(seq_rep)\n","        loss = criterion(output, target.view(-1, 1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    training_losses.append(avg_loss)\n","\n","    # Test Phase\n","    if epoch % test_interval == 0 or epoch == epoch_num - 1:\n","        model.eval()\n","        y_pred_test = []\n","        y_real_test = []\n","        for one_seq_ppt_group in test_loader:\n","            seq_rep, target = one_seq_ppt_group[\"seqs_embeddings\"], one_seq_ppt_group[\"y_property\"]\n","            seq_rep, target = seq_rep.float().to(device), target.float().to(device)\n","\n","            output, _ = model(seq_rep)\n","            output = output.cpu().detach().numpy().reshape(-1)\n","            target = target.cpu().numpy()\n","\n","            y_pred_test.extend(output)\n","            y_real_test.extend(target)\n","\n","        _, _, r_value_test, _, _ = scipy.stats.linregress(y_pred_test, y_real_test)\n","\n","        # Save best model\n","        if r_value_test >= best_r_score:\n","            best_r_score = r_value_test\n","            os.makedirs(save_path, exist_ok=True)\n","            if best_model_file:\n","                os.remove(best_model_file)\n","                os.remove(best_plot_file)\n","                os.remove(best_metrics_file)\n","\n","            # Update file names\n","            best_model_file = f'{save_path}/best_model_epoch_{epoch}_seed_{seed}.pt'\n","            best_plot_file = f'{save_path}/test_plot_epoch_{epoch}.png'\n","            best_metrics_file = f'{save_path}/metrics_epoch_{epoch}.txt'\n","            torch.save(model.state_dict(), best_model_file)\n","            save_plot(y_pred_test, y_real_test, epoch, save_path, best_plot_file.split('/')[-1])\n","            with open(best_metrics_file, 'w') as f:\n","                f.write(f'Epoch: {epoch}\\n')\n","                f.write(f'Test R-value: {r_value_test}\\n')\n","                f.write(f'Test MAE: {mean_absolute_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test MSE: {mean_squared_error(y_pred_test, y_real_test)}\\n')\n","                f.write(f'Test RMSE: {np.sqrt(mean_squared_error(y_pred_test, y_real_test))}\\n')\n","                f.write(f'Test R2: {r2_score(y_real_test, y_pred_test)}\\n')\n","                f.write(f'Test Spearman rho: {scipy.stats.spearmanr(y_pred_test, y_real_test)[0]}\\n')\n","                # Additional metrics can be added here\n","            print(f'Best R-value: {best_r_score:.4f}')\n","\n","    print(f'Epoch: {epoch} | Training Loss: {avg_loss:.4f}')"],"metadata":{"id":"GWRykPU1O3c0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d31c45e2-3dab-4f44-a4ec-2153def7a91f","executionInfo":{"status":"ok","timestamp":1706402598520,"user_tz":-330,"elapsed":112150,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Best R-value: 0.3359\n","Epoch: 0 | Training Loss: 1.6724\n","Epoch: 1 | Training Loss: 1.2096\n","Epoch: 2 | Training Loss: 1.1872\n","Epoch: 3 | Training Loss: 1.1767\n","Epoch: 4 | Training Loss: 1.1631\n","Best R-value: 0.5506\n","Epoch: 5 | Training Loss: 1.1552\n","Epoch: 6 | Training Loss: 1.1163\n","Epoch: 7 | Training Loss: 1.0695\n","Epoch: 8 | Training Loss: 1.0137\n","Epoch: 9 | Training Loss: 0.9204\n","Best R-value: 0.6069\n","Epoch: 10 | Training Loss: 0.8480\n","Epoch: 11 | Training Loss: 0.8126\n","Epoch: 12 | Training Loss: 0.7663\n","Epoch: 13 | Training Loss: 0.6922\n","Epoch: 14 | Training Loss: 0.6459\n","Best R-value: 0.6970\n","Epoch: 15 | Training Loss: 0.6054\n","Epoch: 16 | Training Loss: 0.5835\n","Epoch: 17 | Training Loss: 0.5620\n","Epoch: 18 | Training Loss: 0.5423\n","Epoch: 19 | Training Loss: 0.5178\n","Best R-value: 0.7406\n","Epoch: 20 | Training Loss: 0.5218\n","Epoch: 21 | Training Loss: 0.4864\n","Epoch: 22 | Training Loss: 0.4660\n","Epoch: 23 | Training Loss: 0.4584\n","Epoch: 24 | Training Loss: 0.4406\n","Best R-value: 0.7782\n","Epoch: 25 | Training Loss: 0.4525\n","Epoch: 26 | Training Loss: 0.4054\n","Epoch: 27 | Training Loss: 0.3938\n","Epoch: 28 | Training Loss: 0.3756\n","Epoch: 29 | Training Loss: 0.3620\n","Best R-value: 0.7948\n","Epoch: 30 | Training Loss: 0.3604\n","Epoch: 31 | Training Loss: 0.3481\n","Epoch: 32 | Training Loss: 0.3852\n","Epoch: 33 | Training Loss: 0.3559\n","Epoch: 34 | Training Loss: 0.3474\n","Best R-value: 0.8043\n","Epoch: 35 | Training Loss: 0.3145\n","Epoch: 36 | Training Loss: 0.3095\n","Epoch: 37 | Training Loss: 0.2937\n","Epoch: 38 | Training Loss: 0.2998\n","Epoch: 39 | Training Loss: 0.2742\n","Best R-value: 0.8094\n","Epoch: 40 | Training Loss: 0.2814\n","Epoch: 41 | Training Loss: 0.2671\n","Epoch: 42 | Training Loss: 0.2963\n","Epoch: 43 | Training Loss: 0.2569\n","Epoch: 44 | Training Loss: 0.3057\n","Best R-value: 0.8206\n","Epoch: 45 | Training Loss: 0.2640\n","Epoch: 46 | Training Loss: 0.2307\n","Epoch: 47 | Training Loss: 0.2254\n","Epoch: 48 | Training Loss: 0.2205\n","Epoch: 49 | Training Loss: 0.2147\n","Best R-value: 0.8279\n","Epoch: 50 | Training Loss: 0.2110\n","Epoch: 51 | Training Loss: 0.2079\n","Epoch: 52 | Training Loss: 0.2255\n","Epoch: 53 | Training Loss: 0.2084\n","Epoch: 54 | Training Loss: 0.1956\n","Best R-value: 0.8311\n","Epoch: 55 | Training Loss: 0.2061\n","Epoch: 56 | Training Loss: 0.2124\n","Epoch: 57 | Training Loss: 0.1958\n","Epoch: 58 | Training Loss: 0.1935\n","Epoch: 59 | Training Loss: 0.1982\n","Best R-value: 0.8400\n","Epoch: 60 | Training Loss: 0.2233\n","Epoch: 61 | Training Loss: 0.1931\n","Epoch: 62 | Training Loss: 0.1720\n","Epoch: 63 | Training Loss: 0.1724\n","Epoch: 64 | Training Loss: 0.1727\n","Best R-value: 0.8418\n","Epoch: 65 | Training Loss: 0.1728\n","Epoch: 66 | Training Loss: 0.1679\n","Epoch: 67 | Training Loss: 0.1541\n","Epoch: 68 | Training Loss: 0.1561\n","Epoch: 69 | Training Loss: 0.1626\n","Best R-value: 0.8444\n","Epoch: 70 | Training Loss: 0.1550\n","Epoch: 71 | Training Loss: 0.1510\n","Epoch: 72 | Training Loss: 0.1508\n","Epoch: 73 | Training Loss: 0.1488\n","Epoch: 74 | Training Loss: 0.1399\n","Best R-value: 0.8476\n","Epoch: 75 | Training Loss: 0.1356\n","Epoch: 76 | Training Loss: 0.1347\n","Epoch: 77 | Training Loss: 0.1391\n","Epoch: 78 | Training Loss: 0.1282\n","Epoch: 79 | Training Loss: 0.1340\n","Best R-value: 0.8485\n","Epoch: 80 | Training Loss: 0.1252\n","Epoch: 81 | Training Loss: 0.1268\n","Epoch: 82 | Training Loss: 0.1205\n","Epoch: 83 | Training Loss: 0.1333\n","Epoch: 84 | Training Loss: 0.1197\n","Best R-value: 0.8525\n","Epoch: 85 | Training Loss: 0.1259\n","Epoch: 86 | Training Loss: 0.1175\n","Epoch: 87 | Training Loss: 0.1331\n","Epoch: 88 | Training Loss: 0.1157\n","Epoch: 89 | Training Loss: 0.1137\n","Epoch: 90 | Training Loss: 0.1103\n","Epoch: 91 | Training Loss: 0.1070\n","Epoch: 92 | Training Loss: 0.1096\n","Epoch: 93 | Training Loss: 0.1131\n","Epoch: 94 | Training Loss: 0.1169\n","Best R-value: 0.8569\n","Epoch: 95 | Training Loss: 0.1135\n","Epoch: 96 | Training Loss: 0.1124\n","Epoch: 97 | Training Loss: 0.1057\n","Epoch: 98 | Training Loss: 0.1013\n","Best R-value: 0.8570\n","Epoch: 99 | Training Loss: 0.0996\n"]}]},{"cell_type":"code","source":["# Save the training plot\n","plt.figure()\n","plt.plot(training_losses)\n","plt.xlabel('Epoch')\n","plt.ylabel('Training Loss')\n","plt.title('Training Loss Over Epochs')\n","plt.savefig(f'{save_path}/training_loss_plot_seed_{seed}.png')\n","plt.close()\n","\n","print(f'Training Completed. Best Model Saved with R-score: {best_r_score}')"],"metadata":{"id":"N3I7mIPBO6xs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706402598521,"user_tz":-330,"elapsed":5,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}},"outputId":"62c29283-771d-4c56-c53c-d318ec36a04c"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Completed. Best Model Saved with R-score: 0.8569787486198672\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZWpf3BerPb0a","executionInfo":{"status":"ok","timestamp":1706402598521,"user_tz":-330,"elapsed":3,"user":{"displayName":"LMSE Computational","userId":"11856621797097047403"}}},"execution_count":22,"outputs":[]}]}